{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEt2gXhRcdm0"
      },
      "source": [
        "# FIT3164 - MDS08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMxCv1acsSW"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbmj56OHY1nM",
        "outputId": "45252693-f0bb-41c6-c6ab-47327b4954ad"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSbcvBzVjYWI"
      },
      "source": [
        "### Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoB3qXjgoyy5",
        "outputId": "5fc01863-b3e9-4c6d-8e78-f1e1776aa1d0"
      },
      "outputs": [],
      "source": [
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piEj3SflAqt3",
        "outputId": "b2137256-b20b-449c-e118-8ce180aa2a89"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CMXs-SdgDGre",
        "outputId": "5597bbb2-fbe3-43df-d9bb-5a7889df66e2"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning omegaconf torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "240M7LO2Osnb",
        "outputId": "cb14612f-2a05-4a91-ea40-286537847602"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFcfaOFJZ6yk",
        "outputId": "76bb7cc5-4608-40a9-8d40-2f59e5012f7c"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4RvTkBdoDb1"
      },
      "source": [
        "## Fine-tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWX93w6Uh5vX"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in d:\\github\\fit3164\\.venv\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: numpy in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchvision) (2.1.1)\n",
            "Requirement already satisfied: torch==2.4.1 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchvision) (2.4.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch==2.4.1->torchvision) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in d:\\github\\fit3164\\.venv\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchmetrics) (2.1.1)\n",
            "Requirement already satisfied: packaging>17.1 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchmetrics) (2.4.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torchmetrics) (0.11.7)\n",
            "Requirement already satisfied: setuptools in d:\\github\\fit3164\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\github\\fit3164\\.venv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\github\\fit3164\\.venv\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\github\\fit3164\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "izmp_R6OehVU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import transforms\n",
        "from torchmetrics.classification import MultilabelAccuracy, MultilabelF1Score, MultilabelPrecision, MultilabelRecall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cVAa4IuneiYi"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KgrY_JPuejR8"
      },
      "outputs": [],
      "source": [
        "from transformers import VivitModel, VivitImageProcessor, ViTImageProcessor, ViTForImageClassification, ViTModel, ViTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rlbwe3uzgLEz"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-r44Cqmwen9r"
      },
      "outputs": [],
      "source": [
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nqvM71t4ew8q"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K-wfbXewgqsq"
      },
      "outputs": [],
      "source": [
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OpZ9NXu6gwfM"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1PZLEvjP9MqC"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2Mi0Yi8jg2y-"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "63gP8zp2I_q3"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JU2pYpFvG7aS"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRmLS2Xp9g7n"
      },
      "source": [
        "### Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O1gSHK1dY91E"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ktFjrHoDb2",
        "outputId": "bb9f4cc9-13f5-48e3-f61e-44f10e1ffdc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yDTvjTPGLSgM"
      },
      "outputs": [],
      "source": [
        "HUGGING_MODEL_NAME = 'google/vivit-b-16x2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCbjWcZQ9oUu"
      },
      "source": [
        "### Config, Classes, Collate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "34xrJS1EF7dg"
      },
      "outputs": [],
      "source": [
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c82ToXgAF_gb"
      },
      "outputs": [],
      "source": [
        "config = load_config(\"D:\\GitHub\\FIT3164\\model\\dummy.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cSUwyDdaGESt"
      },
      "outputs": [],
      "source": [
        "def load_classes(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        classes = [row[0] for row in reader]\n",
        "    return classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wZgno8B-GFQE"
      },
      "outputs": [],
      "source": [
        "class_names = load_classes(config[\"dataset\"][\"classes_filepath\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mwX5P56za7LE"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    x_batch, y_batch = zip(*batch)\n",
        "    x_batch = torch.stack(x_batch)\n",
        "    y_batch = torch.stack(y_batch)\n",
        "    return x_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x17OP12h_v4"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYAtB2b1v0Xq"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZFS0CjNKjyGs"
      },
      "outputs": [],
      "source": [
        "def multi_label_to_index(classes, target_labels):\n",
        "    class_to_index = {word: i for i, word in enumerate(classes)}\n",
        "    indexes = [class_to_index[word] for word in target_labels.strip().split() if word in class_to_index]\n",
        "    return torch.tensor(indexes, dtype=torch.int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oQ2BuMUVk9qQ"
      },
      "outputs": [],
      "source": [
        "def pad_video(x, target_length):\n",
        "    current_length = x.size(0)\n",
        "    if current_length >= target_length:\n",
        "        return x[:target_length]\n",
        "    else:\n",
        "        padding_size = target_length - current_length\n",
        "        return F.pad(x, (0, 0, 0, 0, 0, 0, 0, padding_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6KSzFYTv2gf"
      },
      "source": [
        "#### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zb135Yuvn183"
      },
      "outputs": [],
      "source": [
        "class GSL_SI_ViViT(Dataset):\n",
        "    def __init__(self, config, mode, classes_path, processor):\n",
        "        self.config = config['dataset']\n",
        "        self.mode = mode\n",
        "        self.seq_length = self.config[mode]['seq_length']\n",
        "        self.augmentation = self.config[mode]['augmentation']\n",
        "        self.data_path = os.path.join(self.config['input_data'], self.config['images_path'])\n",
        "        self.to_tensor = ToTensorV2()\n",
        "        self.processor = processor\n",
        "\n",
        "        self.indices, self.classes, self.id2w = self.read_gsl_continuous_classes(classes_path)\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "        filepath = self.config[f'{mode}_filepath']\n",
        "        self.list_IDs, self.list_glosses = self.read_gsl_continuous(filepath)\n",
        "        print(f\"{len(self.list_IDs)} {self.mode} instances\")\n",
        "\n",
        "        self.bbox_data = self.bounding_box_handler(self.config['bbox_filepath'])\n",
        "\n",
        "        self.transform = self.get_transforms()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    # def __getitem__(self, index):\n",
        "    #     video_path = self.list_IDs[index]\n",
        "    #     try:\n",
        "    #         frames = self.load_video_sequence(video_path)\n",
        "    #         if len(frames) == 0:\n",
        "    #             print(f\"Skipping index {index} due to empty video sequence\")\n",
        "    #             return self.__getitem__((index + 1) % len(self))\n",
        "    #     except FileNotFoundError as e:\n",
        "    #         print(str(e))\n",
        "    #         return self.__getitem__((index + 1) % len(self))\n",
        "\n",
        "    #     processed_frames = self.process_frames(frames)\n",
        "\n",
        "    #     labels = self.get_labels(self.list_glosses[index])\n",
        "\n",
        "    #     return processed_frames, labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        video_path = self.list_IDs[index]\n",
        "        try:\n",
        "            frames = self.load_video_sequence(video_path)\n",
        "            if len(frames) == 0:\n",
        "                print(f\"Skipping index {index} due to empty video sequence\")\n",
        "                return self.__getitem__((index + 1) % len(self))\n",
        "        except FileNotFoundError as e:\n",
        "            print(str(e))\n",
        "            return self.__getitem__((index + 1) % len(self))\n",
        "\n",
        "        processed_frames = self.process_frames(frames)\n",
        "\n",
        "        labels = self.get_labels(self.list_glosses[index])\n",
        "\n",
        "        return processed_frames, labels\n",
        "\n",
        "    def read_gsl_continuous_classes(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as file:\n",
        "            classes = ['blank'] + file.read().splitlines()\n",
        "\n",
        "        indices = list(range(len(classes)))\n",
        "        id2w = dict(zip(indices, classes))\n",
        "\n",
        "        return indices, classes, id2w\n",
        "\n",
        "    def read_gsl_continuous(self, csv_path):\n",
        "        paths = []\n",
        "        glosses_list = []\n",
        "\n",
        "        with open(csv_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if '|' not in line:\n",
        "                    print(f\"Skipping invalid line: {line}\")\n",
        "                    continue\n",
        "\n",
        "                path, glosses = line.split('|', 1)\n",
        "                paths.append(path)\n",
        "                glosses_list.append(glosses)\n",
        "\n",
        "        return paths, glosses_list\n",
        "\n",
        "    # def read_bounding_box(self, path):\n",
        "    #     bbox = {}\n",
        "    #     with open(path, 'r', encoding='utf-8') as file:\n",
        "    #         for line in file:\n",
        "    #             parts = line.strip().split('|')\n",
        "    #             if len(parts) != 2:\n",
        "    #                 print(f\"Invalid line: {line.strip()}\")\n",
        "    #                 continue\n",
        "\n",
        "    #             video_path, coordinates = parts\n",
        "    #             coords = {k: int(v) for k, v in (coord.strip().split(':') for coord in coordinates.split(','))}\n",
        "    #             bbox[video_path] = coords\n",
        "\n",
        "    #     return bbox\n",
        "\n",
        "    def bounding_box_handler(self, path, video_path=None):\n",
        "        if not hasattr(self, 'bbox_data_cache'):\n",
        "            self.bbox_data_cache = {}\n",
        "            with open(path, 'r', encoding='utf-8') as file:\n",
        "                for line in file:\n",
        "                    parts = line.strip().split('|')\n",
        "                    if len(parts) != 2:\n",
        "                        print(f\"Invalid line: {line.strip()}\")\n",
        "                        continue\n",
        "\n",
        "                    key, coordinates = parts\n",
        "                    coords = {k: int(v) for k, v in (coord.strip().split(':') for coord in coordinates.split(','))}\n",
        "\n",
        "                    self.bbox_data_cache[key] = coords\n",
        "                    self.bbox_data_cache[os.path.basename(key)] = coords\n",
        "                    self.bbox_data_cache[key.replace('/', '_')] = coords\n",
        "\n",
        "        if video_path:\n",
        "            possible_keys = [\n",
        "                video_path,\n",
        "                os.path.basename(video_path),\n",
        "                video_path.replace('/', '_'),\n",
        "                '_'.join(video_path.split('/')[-2:])\n",
        "            ]\n",
        "\n",
        "            for key in possible_keys:\n",
        "                if key in self.bbox_data_cache:\n",
        "                    return self.bbox_data_cache[key]\n",
        "\n",
        "            return None\n",
        "\n",
        "        return self.bbox_data_cache\n",
        "\n",
        "    def get_transforms(self):\n",
        "        if self.augmentation:\n",
        "            return A.Compose([\n",
        "                A.Resize(256, 256),\n",
        "                A.RandomCrop(224, 224),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                A.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
        "            ])\n",
        "        else:\n",
        "            return A.Compose([A.Resize(224, 224)])\n",
        "\n",
        "    # no augmentation\n",
        "    # def get_transforms(self):\n",
        "    #     return A.Compose([A.Resize(224, 224)])\n",
        "\n",
        "    def get_labels(self, gloss_sequence):\n",
        "        class_to_index = {word: i for i, word in enumerate(self.classes)}\n",
        "        indexes = [class_to_index[word] for word in gloss_sequence.strip().split() if word in class_to_index]\n",
        "        labels_tensor = torch.zeros(self.num_classes, dtype=torch.float32)\n",
        "        labels_tensor[indexes] = 1.0\n",
        "        return labels_tensor\n",
        "\n",
        "    def load_video_sequence(self, path, img_type=\"jpg\"):\n",
        "        image_dir = os.path.join(self.data_path, path)\n",
        "        images = sorted(glob.glob(os.path.join(image_dir, f'*.{img_type}')))\n",
        "        total_frames = len(images)\n",
        "\n",
        "        if total_frames == 0:\n",
        "            raise FileNotFoundError(f\"No frames found for video {path}\")\n",
        "\n",
        "        indices = np.linspace(0, total_frames - 1, self.seq_length, dtype=int)\n",
        "        selected_images = [images[i] for i in indices]\n",
        "\n",
        "        frames = []\n",
        "        for img_path in selected_images:\n",
        "            frame = cv2.imread(img_path)\n",
        "            if frame is None:\n",
        "                print(f\"Warning: Failed to read frame {img_path}\")\n",
        "                continue\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            bbox_coords = self.bounding_box_handler(self.config['bbox_filepath'], path)\n",
        "\n",
        "            if bbox_coords:\n",
        "                x1, y1, x2, y2 = bbox_coords['x1'], bbox_coords['y1'], bbox_coords['x2'], bbox_coords['y2']\n",
        "                frame = frame[y1:y2, x1:x2]\n",
        "\n",
        "            frame = self.transform(image=frame)['image']\n",
        "            frame = self.to_tensor(image=frame)['image']\n",
        "            frames.append(frame)\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def process_frames(self, frames):\n",
        "        if len(frames) < self.seq_length:\n",
        "            frames = frames + [frames[-1]] * (self.seq_length - len(frames))\n",
        "        elif len(frames) > self.seq_length:\n",
        "            frames = frames[:self.seq_length]\n",
        "\n",
        "        frames_tensor = torch.stack(frames).float()\n",
        "        return frames_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzfkiiGGi3xf"
      },
      "source": [
        "### Initialise DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f59dbfce13e4d8bb65a47ce016d4bcd",
            "8caa0a37ea5f43f4a64077471c740762",
            "2bd5c94ccb2047ef8ecf333fa3212cbb",
            "775383cc44224f2d84a7d6961ee2b326",
            "314796d7db5942db98b4b56c3ada0016",
            "a7a8c739aa264bbf954b9aed0b839081",
            "7dfa53485ed4450ea0cff16911395ea0",
            "725d3a68754d473586cfea34670ff20a",
            "0acf5732c22b4bc4813a7b1a082acfd1",
            "6085c8ff3c584d86a25e8ef8c045cf34",
            "2d556c07dcea4a74b03ea014f82de176"
          ]
        },
        "id": "mZqfsCVOLag_",
        "outputId": "22a401f2-a634-488c-b55b-a93eca9b4ee2"
      },
      "outputs": [],
      "source": [
        "classes_path = \"D:\\\\GitHub\\\\FIT3164\\\\model\\\\files\\\\GSL_continuous\\\\classes.csv\"\n",
        "processor = VivitImageProcessor.from_pretrained(HUGGING_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7svJZXK_LbPN",
        "outputId": "e441c05b-a514-4552-b4cc-c341922fc6b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8821 train instances\n"
          ]
        }
      ],
      "source": [
        "train_dataset = GSL_SI_ViViT(config, 'train', classes_path, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9sFBE-bsrD",
        "outputId": "fd5529f6-5aba-454a-f4fd-471913151337"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"dataset\"][\"train\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"train\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"train\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aogcscf2LkKW",
        "outputId": "3423a347-f0f2-4a22-a58b-0f9869735b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "588 validation instances\n"
          ]
        }
      ],
      "source": [
        "val_dataset = GSL_SI_ViViT(config, 'validation', classes_path, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SOv9-mRIbvnA"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"dataset\"][\"validation\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"validation\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"validation\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Nl21rFTUc70I"
      },
      "outputs": [],
      "source": [
        "model_num_classes = train_dataset.num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DVxa3hiiCdN"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqcAZom676r0"
      },
      "source": [
        "#### ViViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HpbkJ0kS773n"
      },
      "outputs": [],
      "source": [
        "class GSLViViT(pl.LightningModule):\n",
        "    def __init__(self, num_classes, config):\n",
        "        super(GSLViViT, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.config = config\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # self.vivit = VivitModel.from_pretrained(HUGGING_MODEL_NAME)\n",
        "\n",
        "        self.vivit = VivitModel.from_pretrained(\n",
        "            HUGGING_MODEL_NAME,\n",
        "            hidden_dropout_prob=0.8,\n",
        "            attention_probs_dropout_prob=0.8\n",
        "        )\n",
        "\n",
        "        self.vivit.config.patch_size = self.vivit.config.tubelet_size[1]\n",
        "        self.vivit.config.num_labels = self.num_classes\n",
        "\n",
        "        if hasattr(self.vivit, 'classifier'):\n",
        "            self.vivit.classifier = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Linear(self.vivit.config.hidden_size, self.num_classes)\n",
        "\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.train_accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
        "        self.val_accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
        "        self.test_accuracy = MultilabelAccuracy(num_labels=num_classes)\n",
        "        self.f1_score = MultilabelF1Score(num_labels=num_classes)\n",
        "        self.precision = MultilabelPrecision(num_labels=num_classes)\n",
        "        self.recall = MultilabelRecall(num_labels=num_classes)\n",
        "\n",
        "        self.image_processor = VivitImageProcessor.from_pretrained(HUGGING_MODEL_NAME)\n",
        "\n",
        "        self.freeze()\n",
        "\n",
        "    def freeze(self):\n",
        "        for name, param in self.vivit.named_parameters():\n",
        "            if 'layernorm' in name or 'attention' in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.vivit(pixel_values=pixel_values, interpolate_pos_encoding=True)\n",
        "        logits = self.classifier(outputs.pooler_output)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pixel_values, labels = self.process_batch(batch)\n",
        "        logits = self(pixel_values)\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        preds = torch.sigmoid(logits)\n",
        "        acc = self.train_accuracy(preds, labels.int())\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        pixel_values, labels = self.process_batch(batch)\n",
        "        logits = self(pixel_values)\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        preds = torch.sigmoid(logits)\n",
        "        acc = self.val_accuracy(preds, labels.int())\n",
        "        f1 = self.f1_score(preds, labels.int())\n",
        "        precision = self.precision(preds, labels.int())\n",
        "        recall = self.recall(preds, labels.int())\n",
        "\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_f1', f1, on_step=True, on_epoch=True)\n",
        "        self.log('val_precision', precision, on_step=True, on_epoch=True)\n",
        "        self.log('val_recall', recall, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        pixel_values, labels = self.process_batch(batch)\n",
        "        logits = self(pixel_values)\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        preds = torch.sigmoid(logits)\n",
        "        acc = self.test_accuracy(preds, labels.int())\n",
        "        f1 = self.f1_score(preds, labels.int())\n",
        "        precision = self.precision(preds, labels.int())\n",
        "        recall = self.recall(preds, labels.int())\n",
        "\n",
        "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
        "        self.log('test_acc', acc, on_step=True, on_epoch=True)\n",
        "        self.log('test_f1', f1, on_step=True, on_epoch=True)\n",
        "        self.log('test_precision', precision, on_step=True, on_epoch=True)\n",
        "        self.log('test_recall', recall, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config[\"trainer\"][\"optimizer\"][\"lr\"],\n",
        "            weight_decay=self.config[\"trainer\"][\"optimizer\"][\"weight_decay\"],\n",
        "        )\n",
        "\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=self.config[\"trainer\"][\"scheduler\"][\"max_lr\"],\n",
        "            total_steps=self.trainer.estimated_stepping_batches,\n",
        "            pct_start=self.config[\"trainer\"][\"scheduler\"][\"pct_start\"],\n",
        "            anneal_strategy=self.config[\"trainer\"][\"scheduler\"][\"anneal_strategy\"],\n",
        "            cycle_momentum=self.config[\"trainer\"][\"scheduler\"][\"cycle_momentum\"],\n",
        "            div_factor=self.config[\"trainer\"][\"scheduler\"][\"div_factor\"],\n",
        "            final_div_factor=self.config[\"trainer\"][\"scheduler\"][\"final_div_factor\"],\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def process_batch(self, batch):\n",
        "        x, y = batch\n",
        "        pixel_values = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        return pixel_values, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBheCVuajFA0"
      },
      "source": [
        "### Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "734928e448294f2a82bcb74d2679f1a9",
            "9e5ae59312864e7ba299ef638e6177d1",
            "aef720829dd5434d9995c28c8d1d97e5",
            "2da38c1fe3e64510a6aba3056c35f2d6",
            "805d88bd4c78419a9a323d7e1442621a",
            "9a49080cdbaa4ba9a2ed2e93f7e34e59",
            "27d8c14a6cea4319bbecaefec518d831",
            "1973c8de53464d64937a564fc0e439d3",
            "f13f890ee9fc42adafe5a8349133268f",
            "193172ab8c3c4befa077ceebef87f20a",
            "499e255439be40a6a96ad779cde622b6",
            "fb201b63cad34858a5d7c2bc34426ca1",
            "877e7ec4dcd34e6b9542cb81281ebcbf",
            "5f024b39ed764fd3993424153f755064",
            "6854f158dcbc4308b9cdb9e63ad24cce",
            "84112763c29d4174af8da747c9e79960",
            "e14358a6f88d4932bfd3b3db6d3132bd",
            "d2a4cd1400f24a7ca894189aa20a41e1",
            "6820f2ba3c954964a3df10bcf730ff48",
            "165ac8a0ed024e61b43f6449e226fe5a",
            "63a2c2973c294180836f9175eb6e42b8",
            "4774dd284eb7453dab1671008487cd59"
          ]
        },
        "id": "MhY7AWVFoDb5",
        "outputId": "1adf6836-602b-41bf-f7aa-d1a60a9d9540"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = GSLViViT(num_classes=model_num_classes, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tfoJ84766G2p"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'D:\\GitHub\\FIT3164\\model\\last-v1.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlaWmIPbvzzd",
        "outputId": "fc4a1904-15d1-426a-eca2-d4e207ab781d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GSLViViT(\n",
              "  (vivit): VivitModel(\n",
              "    (embeddings): VivitEmbeddings(\n",
              "      (patch_embeddings): VivitTubeletEmbeddings(\n",
              "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.8, inplace=False)\n",
              "    )\n",
              "    (encoder): VivitEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x VivitLayer(\n",
              "          (attention): VivitAttention(\n",
              "            (attention): VivitSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.8, inplace=False)\n",
              "            )\n",
              "            (output): VivitSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.8, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): VivitIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.8, inplace=False)\n",
              "            (intermediate_act_fn): FastGELUActivation()\n",
              "          )\n",
              "          (output): VivitOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.8, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (pooler): VivitPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=311, bias=True)\n",
              "  (loss_fn): BCEWithLogitsLoss()\n",
              "  (train_accuracy): MultilabelAccuracy()\n",
              "  (val_accuracy): MultilabelAccuracy()\n",
              "  (test_accuracy): MultilabelAccuracy()\n",
              "  (f1_score): MultilabelF1Score()\n",
              "  (precision): MultilabelPrecision()\n",
              "  (recall): MultilabelRecall()\n",
              ")"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GSLViViT.load_from_checkpoint(checkpoint_path)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v2puvawzTEW",
        "outputId": "759b7639-904d-40ab-ce6e-881a0e019d75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GSLViViT(\n",
              "  (vivit): VivitModel(\n",
              "    (embeddings): VivitEmbeddings(\n",
              "      (patch_embeddings): VivitTubeletEmbeddings(\n",
              "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.8, inplace=False)\n",
              "    )\n",
              "    (encoder): VivitEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x VivitLayer(\n",
              "          (attention): VivitAttention(\n",
              "            (attention): VivitSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.8, inplace=False)\n",
              "            )\n",
              "            (output): VivitSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.8, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): VivitIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.8, inplace=False)\n",
              "            (intermediate_act_fn): FastGELUActivation()\n",
              "          )\n",
              "          (output): VivitOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.8, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (pooler): VivitPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=311, bias=True)\n",
              "  (loss_fn): BCEWithLogitsLoss()\n",
              "  (train_accuracy): MultilabelAccuracy()\n",
              "  (val_accuracy): MultilabelAccuracy()\n",
              "  (test_accuracy): MultilabelAccuracy()\n",
              "  (f1_score): MultilabelF1Score()\n",
              "  (precision): MultilabelPrecision()\n",
              "  (recall): MultilabelRecall()\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "R79vSnBpztEP"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.utilities.model_summary import summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eMqQDQmzt-M",
        "outputId": "c3a1c2f5-23c4-4d13-8f97-4fe65cd02915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    | Name                                                    | Type                   | Params | Mode\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "0   | vivit                                                   | VivitModel             | 89.2 M | eval\n",
            "1   | vivit.embeddings                                        | VivitEmbeddings        | 3.6 M  | eval\n",
            "2   | vivit.embeddings.patch_embeddings                       | VivitTubeletEmbeddings | 1.2 M  | eval\n",
            "3   | vivit.embeddings.patch_embeddings.projection            | Conv3d                 | 1.2 M  | eval\n",
            "4   | vivit.embeddings.dropout                                | Dropout                | 0      | eval\n",
            "5   | vivit.encoder                                           | VivitEncoder           | 85.1 M | eval\n",
            "6   | vivit.encoder.layer                                     | ModuleList             | 85.1 M | eval\n",
            "7   | vivit.encoder.layer.0                                   | VivitLayer             | 7.1 M  | eval\n",
            "8   | vivit.encoder.layer.0.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "9   | vivit.encoder.layer.0.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "10  | vivit.encoder.layer.0.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "11  | vivit.encoder.layer.0.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "12  | vivit.encoder.layer.0.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "13  | vivit.encoder.layer.0.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "14  | vivit.encoder.layer.0.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "15  | vivit.encoder.layer.0.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "16  | vivit.encoder.layer.0.attention.output.dropout          | Dropout                | 0      | eval\n",
            "17  | vivit.encoder.layer.0.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "18  | vivit.encoder.layer.0.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "19  | vivit.encoder.layer.0.intermediate.dropout              | Dropout                | 0      | eval\n",
            "20  | vivit.encoder.layer.0.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "21  | vivit.encoder.layer.0.output                            | VivitOutput            | 2.4 M  | eval\n",
            "22  | vivit.encoder.layer.0.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "23  | vivit.encoder.layer.0.output.dropout                    | Dropout                | 0      | eval\n",
            "24  | vivit.encoder.layer.0.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "25  | vivit.encoder.layer.0.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "26  | vivit.encoder.layer.1                                   | VivitLayer             | 7.1 M  | eval\n",
            "27  | vivit.encoder.layer.1.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "28  | vivit.encoder.layer.1.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "29  | vivit.encoder.layer.1.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "30  | vivit.encoder.layer.1.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "31  | vivit.encoder.layer.1.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "32  | vivit.encoder.layer.1.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "33  | vivit.encoder.layer.1.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "34  | vivit.encoder.layer.1.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "35  | vivit.encoder.layer.1.attention.output.dropout          | Dropout                | 0      | eval\n",
            "36  | vivit.encoder.layer.1.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "37  | vivit.encoder.layer.1.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "38  | vivit.encoder.layer.1.intermediate.dropout              | Dropout                | 0      | eval\n",
            "39  | vivit.encoder.layer.1.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "40  | vivit.encoder.layer.1.output                            | VivitOutput            | 2.4 M  | eval\n",
            "41  | vivit.encoder.layer.1.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "42  | vivit.encoder.layer.1.output.dropout                    | Dropout                | 0      | eval\n",
            "43  | vivit.encoder.layer.1.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "44  | vivit.encoder.layer.1.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "45  | vivit.encoder.layer.2                                   | VivitLayer             | 7.1 M  | eval\n",
            "46  | vivit.encoder.layer.2.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "47  | vivit.encoder.layer.2.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "48  | vivit.encoder.layer.2.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "49  | vivit.encoder.layer.2.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "50  | vivit.encoder.layer.2.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "51  | vivit.encoder.layer.2.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "52  | vivit.encoder.layer.2.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "53  | vivit.encoder.layer.2.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "54  | vivit.encoder.layer.2.attention.output.dropout          | Dropout                | 0      | eval\n",
            "55  | vivit.encoder.layer.2.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "56  | vivit.encoder.layer.2.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "57  | vivit.encoder.layer.2.intermediate.dropout              | Dropout                | 0      | eval\n",
            "58  | vivit.encoder.layer.2.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "59  | vivit.encoder.layer.2.output                            | VivitOutput            | 2.4 M  | eval\n",
            "60  | vivit.encoder.layer.2.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "61  | vivit.encoder.layer.2.output.dropout                    | Dropout                | 0      | eval\n",
            "62  | vivit.encoder.layer.2.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "63  | vivit.encoder.layer.2.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "64  | vivit.encoder.layer.3                                   | VivitLayer             | 7.1 M  | eval\n",
            "65  | vivit.encoder.layer.3.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "66  | vivit.encoder.layer.3.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "67  | vivit.encoder.layer.3.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "68  | vivit.encoder.layer.3.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "69  | vivit.encoder.layer.3.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "70  | vivit.encoder.layer.3.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "71  | vivit.encoder.layer.3.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "72  | vivit.encoder.layer.3.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "73  | vivit.encoder.layer.3.attention.output.dropout          | Dropout                | 0      | eval\n",
            "74  | vivit.encoder.layer.3.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "75  | vivit.encoder.layer.3.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "76  | vivit.encoder.layer.3.intermediate.dropout              | Dropout                | 0      | eval\n",
            "77  | vivit.encoder.layer.3.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "78  | vivit.encoder.layer.3.output                            | VivitOutput            | 2.4 M  | eval\n",
            "79  | vivit.encoder.layer.3.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "80  | vivit.encoder.layer.3.output.dropout                    | Dropout                | 0      | eval\n",
            "81  | vivit.encoder.layer.3.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "82  | vivit.encoder.layer.3.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "83  | vivit.encoder.layer.4                                   | VivitLayer             | 7.1 M  | eval\n",
            "84  | vivit.encoder.layer.4.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "85  | vivit.encoder.layer.4.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "86  | vivit.encoder.layer.4.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "87  | vivit.encoder.layer.4.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "88  | vivit.encoder.layer.4.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "89  | vivit.encoder.layer.4.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "90  | vivit.encoder.layer.4.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "91  | vivit.encoder.layer.4.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "92  | vivit.encoder.layer.4.attention.output.dropout          | Dropout                | 0      | eval\n",
            "93  | vivit.encoder.layer.4.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "94  | vivit.encoder.layer.4.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "95  | vivit.encoder.layer.4.intermediate.dropout              | Dropout                | 0      | eval\n",
            "96  | vivit.encoder.layer.4.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "97  | vivit.encoder.layer.4.output                            | VivitOutput            | 2.4 M  | eval\n",
            "98  | vivit.encoder.layer.4.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "99  | vivit.encoder.layer.4.output.dropout                    | Dropout                | 0      | eval\n",
            "100 | vivit.encoder.layer.4.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "101 | vivit.encoder.layer.4.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "102 | vivit.encoder.layer.5                                   | VivitLayer             | 7.1 M  | eval\n",
            "103 | vivit.encoder.layer.5.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "104 | vivit.encoder.layer.5.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "105 | vivit.encoder.layer.5.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "106 | vivit.encoder.layer.5.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "107 | vivit.encoder.layer.5.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "108 | vivit.encoder.layer.5.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "109 | vivit.encoder.layer.5.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "110 | vivit.encoder.layer.5.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "111 | vivit.encoder.layer.5.attention.output.dropout          | Dropout                | 0      | eval\n",
            "112 | vivit.encoder.layer.5.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "113 | vivit.encoder.layer.5.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "114 | vivit.encoder.layer.5.intermediate.dropout              | Dropout                | 0      | eval\n",
            "115 | vivit.encoder.layer.5.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "116 | vivit.encoder.layer.5.output                            | VivitOutput            | 2.4 M  | eval\n",
            "117 | vivit.encoder.layer.5.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "118 | vivit.encoder.layer.5.output.dropout                    | Dropout                | 0      | eval\n",
            "119 | vivit.encoder.layer.5.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "120 | vivit.encoder.layer.5.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "121 | vivit.encoder.layer.6                                   | VivitLayer             | 7.1 M  | eval\n",
            "122 | vivit.encoder.layer.6.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "123 | vivit.encoder.layer.6.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "124 | vivit.encoder.layer.6.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "125 | vivit.encoder.layer.6.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "126 | vivit.encoder.layer.6.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "127 | vivit.encoder.layer.6.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "128 | vivit.encoder.layer.6.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "129 | vivit.encoder.layer.6.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "130 | vivit.encoder.layer.6.attention.output.dropout          | Dropout                | 0      | eval\n",
            "131 | vivit.encoder.layer.6.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "132 | vivit.encoder.layer.6.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "133 | vivit.encoder.layer.6.intermediate.dropout              | Dropout                | 0      | eval\n",
            "134 | vivit.encoder.layer.6.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "135 | vivit.encoder.layer.6.output                            | VivitOutput            | 2.4 M  | eval\n",
            "136 | vivit.encoder.layer.6.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "137 | vivit.encoder.layer.6.output.dropout                    | Dropout                | 0      | eval\n",
            "138 | vivit.encoder.layer.6.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "139 | vivit.encoder.layer.6.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "140 | vivit.encoder.layer.7                                   | VivitLayer             | 7.1 M  | eval\n",
            "141 | vivit.encoder.layer.7.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "142 | vivit.encoder.layer.7.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "143 | vivit.encoder.layer.7.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "144 | vivit.encoder.layer.7.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "145 | vivit.encoder.layer.7.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "146 | vivit.encoder.layer.7.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "147 | vivit.encoder.layer.7.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "148 | vivit.encoder.layer.7.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "149 | vivit.encoder.layer.7.attention.output.dropout          | Dropout                | 0      | eval\n",
            "150 | vivit.encoder.layer.7.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "151 | vivit.encoder.layer.7.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "152 | vivit.encoder.layer.7.intermediate.dropout              | Dropout                | 0      | eval\n",
            "153 | vivit.encoder.layer.7.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "154 | vivit.encoder.layer.7.output                            | VivitOutput            | 2.4 M  | eval\n",
            "155 | vivit.encoder.layer.7.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "156 | vivit.encoder.layer.7.output.dropout                    | Dropout                | 0      | eval\n",
            "157 | vivit.encoder.layer.7.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "158 | vivit.encoder.layer.7.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "159 | vivit.encoder.layer.8                                   | VivitLayer             | 7.1 M  | eval\n",
            "160 | vivit.encoder.layer.8.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "161 | vivit.encoder.layer.8.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "162 | vivit.encoder.layer.8.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "163 | vivit.encoder.layer.8.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "164 | vivit.encoder.layer.8.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "165 | vivit.encoder.layer.8.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "166 | vivit.encoder.layer.8.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "167 | vivit.encoder.layer.8.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "168 | vivit.encoder.layer.8.attention.output.dropout          | Dropout                | 0      | eval\n",
            "169 | vivit.encoder.layer.8.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "170 | vivit.encoder.layer.8.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "171 | vivit.encoder.layer.8.intermediate.dropout              | Dropout                | 0      | eval\n",
            "172 | vivit.encoder.layer.8.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "173 | vivit.encoder.layer.8.output                            | VivitOutput            | 2.4 M  | eval\n",
            "174 | vivit.encoder.layer.8.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "175 | vivit.encoder.layer.8.output.dropout                    | Dropout                | 0      | eval\n",
            "176 | vivit.encoder.layer.8.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "177 | vivit.encoder.layer.8.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "178 | vivit.encoder.layer.9                                   | VivitLayer             | 7.1 M  | eval\n",
            "179 | vivit.encoder.layer.9.attention                         | VivitAttention         | 2.4 M  | eval\n",
            "180 | vivit.encoder.layer.9.attention.attention               | VivitSelfAttention     | 1.8 M  | eval\n",
            "181 | vivit.encoder.layer.9.attention.attention.query         | Linear                 | 590 K  | eval\n",
            "182 | vivit.encoder.layer.9.attention.attention.key           | Linear                 | 590 K  | eval\n",
            "183 | vivit.encoder.layer.9.attention.attention.value         | Linear                 | 590 K  | eval\n",
            "184 | vivit.encoder.layer.9.attention.attention.dropout       | Dropout                | 0      | eval\n",
            "185 | vivit.encoder.layer.9.attention.output                  | VivitSelfOutput        | 590 K  | eval\n",
            "186 | vivit.encoder.layer.9.attention.output.dense            | Linear                 | 590 K  | eval\n",
            "187 | vivit.encoder.layer.9.attention.output.dropout          | Dropout                | 0      | eval\n",
            "188 | vivit.encoder.layer.9.intermediate                      | VivitIntermediate      | 2.4 M  | eval\n",
            "189 | vivit.encoder.layer.9.intermediate.dense                | Linear                 | 2.4 M  | eval\n",
            "190 | vivit.encoder.layer.9.intermediate.dropout              | Dropout                | 0      | eval\n",
            "191 | vivit.encoder.layer.9.intermediate.intermediate_act_fn  | FastGELUActivation     | 0      | eval\n",
            "192 | vivit.encoder.layer.9.output                            | VivitOutput            | 2.4 M  | eval\n",
            "193 | vivit.encoder.layer.9.output.dense                      | Linear                 | 2.4 M  | eval\n",
            "194 | vivit.encoder.layer.9.output.dropout                    | Dropout                | 0      | eval\n",
            "195 | vivit.encoder.layer.9.layernorm_before                  | LayerNorm              | 1.5 K  | eval\n",
            "196 | vivit.encoder.layer.9.layernorm_after                   | LayerNorm              | 1.5 K  | eval\n",
            "197 | vivit.encoder.layer.10                                  | VivitLayer             | 7.1 M  | eval\n",
            "198 | vivit.encoder.layer.10.attention                        | VivitAttention         | 2.4 M  | eval\n",
            "199 | vivit.encoder.layer.10.attention.attention              | VivitSelfAttention     | 1.8 M  | eval\n",
            "200 | vivit.encoder.layer.10.attention.attention.query        | Linear                 | 590 K  | eval\n",
            "201 | vivit.encoder.layer.10.attention.attention.key          | Linear                 | 590 K  | eval\n",
            "202 | vivit.encoder.layer.10.attention.attention.value        | Linear                 | 590 K  | eval\n",
            "203 | vivit.encoder.layer.10.attention.attention.dropout      | Dropout                | 0      | eval\n",
            "204 | vivit.encoder.layer.10.attention.output                 | VivitSelfOutput        | 590 K  | eval\n",
            "205 | vivit.encoder.layer.10.attention.output.dense           | Linear                 | 590 K  | eval\n",
            "206 | vivit.encoder.layer.10.attention.output.dropout         | Dropout                | 0      | eval\n",
            "207 | vivit.encoder.layer.10.intermediate                     | VivitIntermediate      | 2.4 M  | eval\n",
            "208 | vivit.encoder.layer.10.intermediate.dense               | Linear                 | 2.4 M  | eval\n",
            "209 | vivit.encoder.layer.10.intermediate.dropout             | Dropout                | 0      | eval\n",
            "210 | vivit.encoder.layer.10.intermediate.intermediate_act_fn | FastGELUActivation     | 0      | eval\n",
            "211 | vivit.encoder.layer.10.output                           | VivitOutput            | 2.4 M  | eval\n",
            "212 | vivit.encoder.layer.10.output.dense                     | Linear                 | 2.4 M  | eval\n",
            "213 | vivit.encoder.layer.10.output.dropout                   | Dropout                | 0      | eval\n",
            "214 | vivit.encoder.layer.10.layernorm_before                 | LayerNorm              | 1.5 K  | eval\n",
            "215 | vivit.encoder.layer.10.layernorm_after                  | LayerNorm              | 1.5 K  | eval\n",
            "216 | vivit.encoder.layer.11                                  | VivitLayer             | 7.1 M  | eval\n",
            "217 | vivit.encoder.layer.11.attention                        | VivitAttention         | 2.4 M  | eval\n",
            "218 | vivit.encoder.layer.11.attention.attention              | VivitSelfAttention     | 1.8 M  | eval\n",
            "219 | vivit.encoder.layer.11.attention.attention.query        | Linear                 | 590 K  | eval\n",
            "220 | vivit.encoder.layer.11.attention.attention.key          | Linear                 | 590 K  | eval\n",
            "221 | vivit.encoder.layer.11.attention.attention.value        | Linear                 | 590 K  | eval\n",
            "222 | vivit.encoder.layer.11.attention.attention.dropout      | Dropout                | 0      | eval\n",
            "223 | vivit.encoder.layer.11.attention.output                 | VivitSelfOutput        | 590 K  | eval\n",
            "224 | vivit.encoder.layer.11.attention.output.dense           | Linear                 | 590 K  | eval\n",
            "225 | vivit.encoder.layer.11.attention.output.dropout         | Dropout                | 0      | eval\n",
            "226 | vivit.encoder.layer.11.intermediate                     | VivitIntermediate      | 2.4 M  | eval\n",
            "227 | vivit.encoder.layer.11.intermediate.dense               | Linear                 | 2.4 M  | eval\n",
            "228 | vivit.encoder.layer.11.intermediate.dropout             | Dropout                | 0      | eval\n",
            "229 | vivit.encoder.layer.11.intermediate.intermediate_act_fn | FastGELUActivation     | 0      | eval\n",
            "230 | vivit.encoder.layer.11.output                           | VivitOutput            | 2.4 M  | eval\n",
            "231 | vivit.encoder.layer.11.output.dense                     | Linear                 | 2.4 M  | eval\n",
            "232 | vivit.encoder.layer.11.output.dropout                   | Dropout                | 0      | eval\n",
            "233 | vivit.encoder.layer.11.layernorm_before                 | LayerNorm              | 1.5 K  | eval\n",
            "234 | vivit.encoder.layer.11.layernorm_after                  | LayerNorm              | 1.5 K  | eval\n",
            "235 | vivit.layernorm                                         | LayerNorm              | 1.5 K  | eval\n",
            "236 | vivit.pooler                                            | VivitPooler            | 590 K  | eval\n",
            "237 | vivit.pooler.dense                                      | Linear                 | 590 K  | eval\n",
            "238 | vivit.pooler.activation                                 | Tanh                   | 0      | eval\n",
            "239 | classifier                                              | Linear                 | 239 K  | eval\n",
            "240 | loss_fn                                                 | BCEWithLogitsLoss      | 0      | eval\n",
            "241 | train_accuracy                                          | MultilabelAccuracy     | 0      | eval\n",
            "242 | val_accuracy                                            | MultilabelAccuracy     | 0      | eval\n",
            "243 | test_accuracy                                           | MultilabelAccuracy     | 0      | eval\n",
            "244 | f1_score                                                | MultilabelF1Score      | 0      | eval\n",
            "245 | precision                                               | MultilabelPrecision    | 0      | eval\n",
            "246 | recall                                                  | MultilabelRecall       | 0      | eval\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "28.6 M    Trainable params\n",
            "60.9 M    Non-trainable params\n",
            "89.5 M    Total params\n",
            "357.905   Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "247       Modules in eval mode\n"
          ]
        }
      ],
      "source": [
        "model_summary = summarize(model, max_depth=-1)\n",
        "print(model_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8821 train instances\n",
            "Recording started\n",
            "Recording stopped, processing...\n",
            "Input tensor shape: torch.Size([1, 32, 3, 224, 224])\n",
            "Predicted Sentence:\n",
            "ΕΓΩ(1) (0.26)\n",
            "ΕΣΥ (0.24)\n",
            "ΚΑΛΟ (0.11)\n",
            "\n",
            "Predicted classes: ΕΓΩ(1) (0.26) | ΕΣΥ (0.24) | ΚΑΛΟ (0.11)\n",
            "Video stream ended\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "import yaml\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "# Load the config and checkpoint\n",
        "config = load_config(r\"D:\\GitHub\\FIT3164\\model\\configs\\dummy.yaml\")\n",
        "checkpoint_path = r'D:\\GitHub\\FIT3164\\model\\last-v1.ckpt'\n",
        "\n",
        "# Load the model\n",
        "model = GSLViViT.load_from_checkpoint(checkpoint_path)\n",
        "model.eval()\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 32  # From the model config\n",
        "FRAME_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "# Create id2label mapping\n",
        "classes_path = config['dataset']['classes_filepath']\n",
        "dataset = GSL_SI_ViViT(config, 'train', classes_path, None)\n",
        "id2label = {i: label for i, label in enumerate(dataset.classes)}\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.resize(frame, (FRAME_SIZE, FRAME_SIZE))\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = frame.astype(np.float32) / 255.0\n",
        "    frame = (frame - 0.45) / 0.225\n",
        "    return frame\n",
        "\n",
        "def process_sequence(frames):\n",
        "    if len(frames) < SEQUENCE_LENGTH:\n",
        "        frames = frames + [frames[-1]] * (SEQUENCE_LENGTH - len(frames))\n",
        "    elif len(frames) > SEQUENCE_LENGTH:\n",
        "        frames = frames[:SEQUENCE_LENGTH]\n",
        "    \n",
        "    frames = np.stack(frames)\n",
        "    \n",
        "    # Reshape to (batch_size, num_frames, num_channels, height, width)\n",
        "    frames = frames.transpose(0, 3, 1, 2)\n",
        "    frames = torch.from_numpy(frames).unsqueeze(0)\n",
        "    \n",
        "    return frames.to(device)\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(2)\n",
        "\n",
        "frame_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
        "recording = False\n",
        "predicted_class = \"No prediction yet\"\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Display the frame\n",
        "        display_frame = frame.copy()\n",
        "        cv2.putText(display_frame, f\"Predicted: {predicted_class}\", (10, 30), \n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        cv2.putText(display_frame, \"Press 'r' to start/stop recording, 'q' to quit\", \n",
        "                    (10, display_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    0.5, (255, 255, 255), 1)\n",
        "        \n",
        "        cv2.imshow('ViViT GSL Recognition', display_frame)\n",
        "\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "        elif key == ord('r'):\n",
        "            recording = not recording\n",
        "            if recording:\n",
        "                print(\"Recording started\")\n",
        "                frame_buffer.clear()\n",
        "            else:\n",
        "                print(\"Recording stopped, processing...\")\n",
        "                if len(frame_buffer) > 0:\n",
        "                    input_tensor = process_sequence(list(frame_buffer))\n",
        "                    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
        "                    \n",
        "                    try:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(input_tensor)\n",
        "                            probabilities = torch.sigmoid(outputs)\n",
        "                        \n",
        "                        top_probs, top_classes = torch.topk(probabilities, k=5, dim=1)\n",
        "                        predicted_classes = []\n",
        "                        for i in range(3):  # Only iterate over top 5\n",
        "                            class_name = id2label[top_classes[0][i].item()]\n",
        "                            prob = top_probs[0][i].item()\n",
        "                            predicted_classes.append(f\"{class_name} ({prob:.2f})\")\n",
        "                        predicted_class = \" | \".join(predicted_classes)\n",
        "                        print(f\"Predicted Sentence:\")\n",
        "                        for class_prob in predicted_classes:\n",
        "                            print(class_prob)\n",
        "                        print(f\"\\nPredicted classes: {predicted_class}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error during prediction: {str(e)}\")\n",
        "                        print(f\"Model input shape: {input_tensor.shape}\")\n",
        "                else:\n",
        "                    print(\"No frames recorded\")\n",
        "\n",
        "        if recording:\n",
        "            processed_frame = preprocess_frame(frame)\n",
        "            frame_buffer.append(processed_frame)\n",
        "\n",
        "        time.sleep(0.01)  # Small delay to control frame rate\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Interrupted by user\")\n",
        "\n",
        "finally:\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Video stream ended\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dSbcvBzVjYWI",
        "e2LnISI5jaFk",
        "1Lw_WNcMjbtP",
        "XWX93w6Uh5vX",
        "MCbjWcZQ9oUu",
        "ZYAtB2b1v0Xq",
        "-6KSzFYTv2gf",
        "IzfkiiGGi3xf",
        "DqcAZom676r0",
        "rBheCVuajFA0",
        "Zn0l8xmWjEmz",
        "PHuW_AxawSCc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0acf5732c22b4bc4813a7b1a082acfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "165ac8a0ed024e61b43f6449e226fe5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "193172ab8c3c4befa077ceebef87f20a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1973c8de53464d64937a564fc0e439d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d03ee69668f4d8d808aa8fe416778d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bcedbb3c7cb4063b45fffe40451ad84",
            "placeholder": "​",
            "style": "IPY_MODEL_867fe258c119437199be90b0f70a397f",
            "value": " 0/4411 [00:00&lt;?, ?it/s]"
          }
        },
        "1f59dbfce13e4d8bb65a47ce016d4bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8caa0a37ea5f43f4a64077471c740762",
              "IPY_MODEL_2bd5c94ccb2047ef8ecf333fa3212cbb",
              "IPY_MODEL_775383cc44224f2d84a7d6961ee2b326"
            ],
            "layout": "IPY_MODEL_314796d7db5942db98b4b56c3ada0016"
          }
        },
        "27d8c14a6cea4319bbecaefec518d831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd5c94ccb2047ef8ecf333fa3212cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725d3a68754d473586cfea34670ff20a",
            "max": 430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0acf5732c22b4bc4813a7b1a082acfd1",
            "value": 430
          }
        },
        "2d556c07dcea4a74b03ea014f82de176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2da38c1fe3e64510a6aba3056c35f2d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193172ab8c3c4befa077ceebef87f20a",
            "placeholder": "​",
            "style": "IPY_MODEL_499e255439be40a6a96ad779cde622b6",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 1.36MB/s]"
          }
        },
        "314796d7db5942db98b4b56c3ada0016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a0fc930b3b410ba9a83839a7f61449": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4774dd284eb7453dab1671008487cd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "499e255439be40a6a96ad779cde622b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58d0259b2dfa43c1bf414352f7436dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b71dfc3c9ef44b08934602da3487e854",
              "IPY_MODEL_93103ffd3ebb4c2ba4bffcc5a201d83e",
              "IPY_MODEL_1d03ee69668f4d8d808aa8fe416778d0"
            ],
            "layout": "IPY_MODEL_ffaef79d670f4354b4128ce2808805ba"
          }
        },
        "5f024b39ed764fd3993424153f755064": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6820f2ba3c954964a3df10bcf730ff48",
            "max": 355881581,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_165ac8a0ed024e61b43f6449e226fe5a",
            "value": 355881581
          }
        },
        "6085c8ff3c584d86a25e8ef8c045cf34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62fe1d0e67c04045a417193f061b10db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63a2c2973c294180836f9175eb6e42b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6820f2ba3c954964a3df10bcf730ff48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6854f158dcbc4308b9cdb9e63ad24cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a2c2973c294180836f9175eb6e42b8",
            "placeholder": "​",
            "style": "IPY_MODEL_4774dd284eb7453dab1671008487cd59",
            "value": " 356M/356M [00:05&lt;00:00, 81.6MB/s]"
          }
        },
        "725d3a68754d473586cfea34670ff20a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734928e448294f2a82bcb74d2679f1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e5ae59312864e7ba299ef638e6177d1",
              "IPY_MODEL_aef720829dd5434d9995c28c8d1d97e5",
              "IPY_MODEL_2da38c1fe3e64510a6aba3056c35f2d6"
            ],
            "layout": "IPY_MODEL_805d88bd4c78419a9a323d7e1442621a"
          }
        },
        "775383cc44224f2d84a7d6961ee2b326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6085c8ff3c584d86a25e8ef8c045cf34",
            "placeholder": "​",
            "style": "IPY_MODEL_2d556c07dcea4a74b03ea014f82de176",
            "value": " 430/430 [00:00&lt;00:00, 24.5kB/s]"
          }
        },
        "7dfa53485ed4450ea0cff16911395ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "805d88bd4c78419a9a323d7e1442621a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84112763c29d4174af8da747c9e79960": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867fe258c119437199be90b0f70a397f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "877e7ec4dcd34e6b9542cb81281ebcbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14358a6f88d4932bfd3b3db6d3132bd",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a4cd1400f24a7ca894189aa20a41e1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8caa0a37ea5f43f4a64077471c740762": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a8c739aa264bbf954b9aed0b839081",
            "placeholder": "​",
            "style": "IPY_MODEL_7dfa53485ed4450ea0cff16911395ea0",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "93103ffd3ebb4c2ba4bffcc5a201d83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a0fc930b3b410ba9a83839a7f61449",
            "max": 4411,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e02a288284ab4421a4a6b7a6cec6bb5e",
            "value": 0
          }
        },
        "9a49080cdbaa4ba9a2ed2e93f7e34e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bcedbb3c7cb4063b45fffe40451ad84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5ae59312864e7ba299ef638e6177d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a49080cdbaa4ba9a2ed2e93f7e34e59",
            "placeholder": "​",
            "style": "IPY_MODEL_27d8c14a6cea4319bbecaefec518d831",
            "value": "config.json: 100%"
          }
        },
        "a7a8c739aa264bbf954b9aed0b839081": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aef720829dd5434d9995c28c8d1d97e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1973c8de53464d64937a564fc0e439d3",
            "max": 22679,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f13f890ee9fc42adafe5a8349133268f",
            "value": 22679
          }
        },
        "b71dfc3c9ef44b08934602da3487e854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cde05eeb8f9e4bf7bc5cf0a290cdcb3d",
            "placeholder": "​",
            "style": "IPY_MODEL_62fe1d0e67c04045a417193f061b10db",
            "value": "Epoch 0:   0%"
          }
        },
        "cde05eeb8f9e4bf7bc5cf0a290cdcb3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a4cd1400f24a7ca894189aa20a41e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e02a288284ab4421a4a6b7a6cec6bb5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e14358a6f88d4932bfd3b3db6d3132bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f13f890ee9fc42adafe5a8349133268f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb201b63cad34858a5d7c2bc34426ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_877e7ec4dcd34e6b9542cb81281ebcbf",
              "IPY_MODEL_5f024b39ed764fd3993424153f755064",
              "IPY_MODEL_6854f158dcbc4308b9cdb9e63ad24cce"
            ],
            "layout": "IPY_MODEL_84112763c29d4174af8da747c9e79960"
          }
        },
        "ffaef79d670f4354b4128ce2808805ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
