{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEt2gXhRcdm0"
      },
      "source": [
        "# FIT3164 - MDS08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMxCv1acsSW"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment if using cpu training\n",
        "# !pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo8AJ5pXdUys"
      },
      "source": [
        "## Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install zenodo_get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLzqHMY9LP-C"
      },
      "outputs": [],
      "source": [
        "!zenodo_get 4756317"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all imports\n",
        "import os\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "import cv2\n",
        "import logging\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import yaml\n",
        "import csv\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "from transformers import ViTImageProcessor, ViTModel, ViTConfig\n",
        "\n",
        "\n",
        "import tensorboard\n",
        "\n",
        "\n",
        "import tensorboardX\n",
        "\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "from base.base_loader import BaseDataset\n",
        "\n",
        "\n",
        "from datasets.loader_utils import (\n",
        "    multi_label_to_index,\n",
        "    pad_video,\n",
        "    video_transforms,\n",
        "    sampling,\n",
        "\n",
        "    VideoRandomResizedCrop,\n",
        "    read_gsl_continuous,\n",
        "\n",
        "    gsl_context,\n",
        "    read_bounding_box,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if cuda is available\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment and run this cell to use cpu\n",
        "# torch.cuda.is_available = lambda : False\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if cuda is available (should say False if the above cell is run)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set random seed\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reference the vision transformer that will be used (86 million params)\n",
        "HUGGING_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
        "\n",
        "\n",
        "# HUGGING_MODEL_NAME = 'google/vit-large-patch16-224-in21k'\n",
        "\n",
        "\n",
        "# HUGGING_MODEL_NAME = 'google/vit-huge-patch14-224-in21k'\n",
        "\n",
        "\n",
        "\n",
        "# BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# MAX_EPOCHS = 10\n",
        "\n",
        "\n",
        "# LEARNING_RATE = 3e-4\n",
        "\n",
        "\n",
        "# SEQUENCE_LENGTH = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set image processor\n",
        "image_processor = ViTImageProcessor.from_pretrained(HUGGING_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# too much work to remove args from the code, so just create a dummy class and pass it to the functions\n",
        "class Args:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.cwd = os.getcwd()\n",
        "\n",
        "        self.input_data = config[\"dataset\"][\"input_data\"]\n",
        "\n",
        "        self.return_context = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set file paths for fine tuning\n",
        "feats_path = \"gsl_cont_features/\"\n",
        "\n",
        "\n",
        "train_prefix = \"train\"\n",
        "\n",
        "\n",
        "dev_prefix = \"dev\"\n",
        "\n",
        "\n",
        "validation_prefix = \"validation\"\n",
        "\n",
        "\n",
        "test_prefix = \"test\"\n",
        "\n",
        "\n",
        "\n",
        "train_filepath = \"files/GSL_continuous/gsl_split_SI_train.csv\"\n",
        "\n",
        "\n",
        "dev_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "\n",
        "\n",
        "validation_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "\n",
        "\n",
        "test_filepath = \"files/GSL_continuous/gsl_split_SI_test.csv\"\n",
        "\n",
        "\n",
        "\n",
        "# dataset class inherits from BaseDataset\n",
        "\n",
        "\n",
        "class GSL_SI(BaseDataset):\n",
        "\n",
        "    def __init__(self, config, args, mode, classes):\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        Args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            config:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            mode:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            classes:\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        super(GSL_SI, self).__init__(config, args, mode, classes)\n",
        "\n",
        "\n",
        "        self.config = config[\"dataset\"]\n",
        "        self.mode = mode\n",
        "\n",
        "\n",
        "        self.dim = tuple(self.config[\"dim\"])\n",
        "        self.classes = classes\n",
        "\n",
        "\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        self.num_classes = self.config[\"classes\"]\n",
        "\n",
        "        self.seq_length = self.config[self.mode][\"seq_length\"]\n",
        "\n",
        "        self.normalize = self.config[\"normalize\"]\n",
        "\n",
        "        self.padding = self.config[\"padding\"]\n",
        "\n",
        "        self.augmentation = self.config[self.mode][\"augmentation\"]\n",
        "\n",
        "        self.return_context = args.return_context\n",
        "\n",
        "\n",
        "        self.rgb_data_path = os.path.join(\n",
        "            self.config[\"input_data\"], self.config[\"images_path\"]\n",
        "        )\n",
        "\n",
        "        self.depth_data_path = os.path.join(\n",
        "            self.config[\"input_data\"], self.config[\"depth_path\"]\n",
        "        )\n",
        "\n",
        "\n",
        "        if self.mode == train_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, train_filepath)\n",
        "            )\n",
        "\n",
        "        elif self.mode == validation_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, validation_filepath)\n",
        "            )\n",
        "\n",
        "        elif self.mode == test_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, test_filepath)\n",
        "            )\n",
        "\n",
        "\n",
        "        print(f\"{len(self.list_IDs)} {self.mode} instances\")\n",
        "\n",
        "\n",
        "        self.bbox = read_bounding_box(\n",
        "            os.path.join(args.cwd, \"files/GSL_continuous/bbox_for_gsl_continuous.txt\")\n",
        "        )\n",
        "\n",
        "\n",
        "        self.context = gsl_context(self.list_IDs, self.list_glosses)\n",
        "\n",
        "\n",
        "        if self.config[\"modality\"] == \"full\":\n",
        "\n",
        "            self.data_path = os.path.join(\n",
        "                self.config[\"input_data\"], self.config[\"images_path\"]\n",
        "            )\n",
        "\n",
        "            self.get = self.video_loader\n",
        "\n",
        "        elif self.config[\"modality\"] == \"features\":\n",
        "\n",
        "            self.data_path = os.path.join(\n",
        "                self.config[\"input_data\"], self.config[\"features_path\"]\n",
        "            )\n",
        "\n",
        "            self.get = self.feature_loader\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        rgb_x = self.load_video_sequence(path=self.list_IDs[index], img_type=\"jpg\")\n",
        "\n",
        "        depth_x = self.load_video_sequence(\n",
        "            path=self.list_IDs[index].replace(\"color\", \"depth\"), img_type=\"jpg\"\n",
        "        )\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"In __getitem__: RGB shape: {rgb_x.shape}\")\n",
        "\n",
        "        print(f\"In __getitem__: Depth shape: {depth_x.shape}\")\n",
        "\n",
        "        print(f\"In __getitem__: Label shape: {y.shape}\")\n",
        "\n",
        "\n",
        "        return rgb_x, depth_x, y\n",
        "\n",
        "\n",
        "    def feature_loader(self, index):\n",
        "\n",
        "        folder_path = os.path.join(self.data_path, self.list_IDs[index])\n",
        "\n",
        "        # print(folder_path)\n",
        "\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "\n",
        "        if self.context[index] != None:\n",
        "\n",
        "\n",
        "            c = multi_label_to_index(\n",
        "                classes=self.classes, target_labels=self.context[index]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "\n",
        "            c = torch.tensor([0], dtype=torch.int)\n",
        "\n",
        "        x = torch.FloatTensor(np.load(folder_path + \".npy\")).squeeze(0)\n",
        "\n",
        "        if self.return_context:\n",
        "\n",
        "            return x, [y, c]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def video_loader(self, index):\n",
        "\n",
        "\n",
        "        x = self.load_video_sequence(path=self.list_IDs[index], img_type=\"jpg\")\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def load_video_sequence(self, path, img_type=\"jpg\", is_depth=False):\n",
        "\n",
        "        data_path = self.depth_data_path if is_depth else self.rgb_data_path\n",
        "\n",
        "        images = sorted(\n",
        "            glob.glob(\n",
        "                os.path.join(\n",
        "                    data_path,\n",
        "                    path,\n",
        "                )\n",
        "                + \"/*\"\n",
        "                + img_type\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        h_flip = False\n",
        "\n",
        "        img_sequence = []\n",
        "\n",
        "        # print(images)\n",
        "\n",
        "        if len(images) < 1:\n",
        "\n",
        "            print(os.path.join(self.data_path, path))\n",
        "\n",
        "        bbox = self.bbox.get(path)\n",
        "\n",
        "\n",
        "        if self.augmentation:\n",
        "\n",
        "            ## training set temporal  AUGMENTATION\n",
        "\n",
        "            temporal_augmentation = int(\n",
        "                (np.random.randint(80, 100) / 100.0) * len(images)\n",
        "            )\n",
        "\n",
        "            if temporal_augmentation > 15:\n",
        "\n",
        "                images = sorted(random.sample(images, k=temporal_augmentation))\n",
        "\n",
        "            if len(images) > self.seq_length:\n",
        "\n",
        "                # random frame sampling\n",
        "\n",
        "                images = sorted(random.sample(images, k=self.seq_length))\n",
        "\n",
        "\n",
        "        else:\n",
        "\n",
        "            # test uniform sampling\n",
        "\n",
        "\n",
        "            if len(images) > self.seq_length:\n",
        "\n",
        "                images = sorted(sampling(images, self.seq_length))\n",
        "\n",
        "\n",
        "        i = np.random.randint(0, 30)\n",
        "\n",
        "        j = np.random.randint(0, 30)\n",
        "\n",
        "        brightness = 1 + random.uniform(-0.2, +0.2)\n",
        "\n",
        "        contrast = 1 + random.uniform(-0.2, +0.2)\n",
        "\n",
        "        hue = random.uniform(0, 1) / 10.0\n",
        "\n",
        "        # r_resize = ((112, 112))\n",
        "\n",
        "        r_resize = (224, 224)\n",
        "\n",
        "        crop_or_bbox = random.uniform(0, 1) > 0.5\n",
        "\n",
        "        to_flip = random.uniform(0, 1) > 1\n",
        "\n",
        "        grayscale = random.uniform(0, 1) > 0.9\n",
        "\n",
        "        t1 = VideoRandomResizedCrop(self.dim[0], scale=(0.9, 1.0), ratio=(0.8, 1.2))\n",
        "\n",
        "        for img_path in images:\n",
        "\n",
        "\n",
        "            frame_o = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "            crop_size = 120\n",
        "\n",
        "            ## CROP BOUNDING BOX\n",
        "\n",
        "            ## CROP BOUNDING BOX\n",
        "\n",
        "\n",
        "            frame1 = np.array(frame_o)\n",
        "\n",
        "\n",
        "            frame1 = frame1[:, crop_size : 648 - crop_size]\n",
        "\n",
        "            frame = Image.fromarray(frame1)\n",
        "\n",
        "\n",
        "            if self.augmentation:\n",
        "\n",
        "\n",
        "                ## training set DATA AUGMENTATION\n",
        "\n",
        "\n",
        "                frame = frame.resize(r_resize)\n",
        "\n",
        "\n",
        "                img_tensor = video_transforms(\n",
        "                    img=frame,\n",
        "                    i=i,\n",
        "                    j=j,\n",
        "                    bright=brightness,\n",
        "                    cont=contrast,\n",
        "                    h=hue,\n",
        "                    dim=self.dim,\n",
        "\n",
        "                    resized_crop=t1,\n",
        "                    augmentation=True,\n",
        "                    normalize=self.normalize,\n",
        "                    crop=crop_or_bbox,\n",
        "                    to_flip=to_flip,\n",
        "                    grayscale=grayscale,\n",
        "                )\n",
        "\n",
        "                img_sequence.append(img_tensor)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # TEST set  NO DATA AUGMENTATION\n",
        "\n",
        "                if is_depth:\n",
        "\n",
        "                    # Resize depth frame to match RGB dimensions\n",
        "\n",
        "                    frame = frame.resize(self.dim)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    frame = frame.resize(self.dim)\n",
        "\n",
        "\n",
        "                img_tensor = video_transforms(\n",
        "                    img=frame,\n",
        "                    i=i,\n",
        "                    j=j,\n",
        "                    bright=1,\n",
        "                    cont=1,\n",
        "                    h=0,\n",
        "                    dim=self.dim,\n",
        "                    augmentation=False,\n",
        "                    normalize=self.normalize,\n",
        "                )\n",
        "\n",
        "                img_sequence.append(img_tensor)\n",
        "\n",
        "        pad_len = self.seq_length - len(images)\n",
        "\n",
        "\n",
        "        X1 = torch.stack(img_sequence).float()\n",
        "\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            X1 = pad_video(X1, padding_size=pad_len, padding_type=\"zeros\")\n",
        "\n",
        "\n",
        "        if len(images) < 25:\n",
        "\n",
        "            X1 = pad_video(X1, padding_size=25 - len(images), padding_type=\"zeros\")\n",
        "\n",
        "        return X1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GSLDualViT(pl.LightningModule):\n",
        "    def __init__(self, num_classes, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.class_names = class_names\n",
        "        self.num_classes = len(class_names)\n",
        "        # self.logger = logger\n",
        "        self.learning_rate = config[\"trainer\"][\"optimizer\"][\"lr\"]\n",
        "        self.sequence_length = config[\"dataset\"][\"train\"][\"seq_length\"]\n",
        "\n",
        "        # RGB ViT\n",
        "        self.rgb_vit = ViTModel.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        self.rgb_vit.train()\n",
        "\n",
        "        for param in self.rgb_vit.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Depth ViT (initialize with same config but different input channels)\n",
        "        depth_config = ViTConfig.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        depth_config.num_channels = 3\n",
        "        self.depth_vit = ViTModel(depth_config)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Linear(\n",
        "            self.rgb_vit.config.hidden_size * 2, self.rgb_vit.config.hidden_size\n",
        "        )\n",
        "\n",
        "        # Temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.rgb_vit.config.hidden_size,\n",
        "            self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"hidden_size\"],\n",
        "            num_layers=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"num_layers\"],\n",
        "            bidirectional=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\n",
        "                \"bidirectional\"\n",
        "            ],\n",
        "            batch_first=True,\n",
        "            dropout=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"dropout\"],\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        lstm_output_size = self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\n",
        "            \"hidden_size\"\n",
        "        ]\n",
        "        if self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"bidirectional\"]:\n",
        "            lstm_output_size *= 2\n",
        "        self.classifier = nn.Linear(lstm_output_size, num_classes)\n",
        "\n",
        "    def forward(self, rgb_sequence, depth_sequence=None):\n",
        "        print(\n",
        "            f\"Forward pass input shapes: RGB {rgb_sequence.shape}, Depth {depth_sequence.shape if depth_sequence is not None else None}\"\n",
        "        )\n",
        "        batch_size, seq_len, channels, height, width = rgb_sequence.shape\n",
        "        fused_features = []\n",
        "        for i in range(seq_len):\n",
        "            print(f\"Processing frame {i}\")\n",
        "            try:\n",
        "                rgb_features = self.rgb_vit(rgb_sequence[:, i]).last_hidden_state[:, 0]\n",
        "                print(f\"RGB features shape: {rgb_features.shape}\")\n",
        "\n",
        "                if depth_sequence is not None:\n",
        "                    depth_features = self.depth_vit(\n",
        "                        depth_sequence[:, i]\n",
        "                    ).last_hidden_state[:, 0]\n",
        "                    print(f\"Depth features shape: {depth_features.shape}\")\n",
        "                    fused = self.fusion(\n",
        "                        torch.cat([rgb_features, depth_features], dim=-1)\n",
        "                    )\n",
        "                else:\n",
        "                    fused = rgb_features\n",
        "\n",
        "                print(f\"Fused features shape: {fused.shape}\")\n",
        "                fused_features.append(fused)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {i}: {str(e)}\")\n",
        "\n",
        "        print(f\"Number of fused features: {len(fused_features)}\")\n",
        "        if len(fused_features) == 0:\n",
        "            print(\"fused_features is empty!\")\n",
        "            return None\n",
        "\n",
        "        fused_sequence = torch.stack(fused_features, dim=1)\n",
        "        print(f\"Fused sequence shape: {fused_sequence.shape}\")\n",
        "        lstm_out, _ = self.lstm(fused_sequence)\n",
        "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
        "        output = self.classifier(lstm_out[:, -1])\n",
        "        print(f\"Final output shape: {output.shape}\")\n",
        "        return fused_sequence  # Changed from 'output' to 'fused_sequence'\n",
        "\n",
        "    # def forward(self, rgb_sequence, depth_sequence=None):\n",
        "    #     batch_size, seq_len, channels, height, width = rgb_sequence.shape\n",
        "    #     print(f\"In forward: RGB shape: {rgb_sequence.shape}\")\n",
        "\n",
        "    # Process each frame in the sequence\n",
        "    # fused_features = []\n",
        "    # for i in range(seq_len):\n",
        "    #     rgb_features = self.rgb_vit(rgb_sequence[:, i]).last_hidden_state[:, 0]  # Use CLS token\n",
        "\n",
        "    #     if depth_sequence is not None:\n",
        "    #         print(f\"In forward: Depth shape: {depth_sequence.shape}\")\n",
        "    #         depth_sequence_resized = F.interpolate(depth_sequence[:, i], size=(height, width), mode='bilinear', align_corners=False)\n",
        "    #         depth_features = self.depth_vit(depth_sequence_resized).last_hidden_state[:, 0]  # Use CLS token\n",
        "    #         fused = self.fusion(torch.cat([rgb_features, depth_features], dim=-1))\n",
        "    #     else:\n",
        "    #         fused = rgb_features\n",
        "\n",
        "    #     fused_features.append(fused)\n",
        "\n",
        "    # fused_sequence = torch.stack(fused_features, dim=1)\n",
        "\n",
        "    # Temporal modeling\n",
        "    # lstm_out, _ = self.lstm(fused_sequence)\n",
        "\n",
        "    # Classification\n",
        "    # output = self.classifier(lstm_out[:, -1])  # Use last timestep for classification\n",
        "    # print(f\"In forward: Output shape: {output.shape}\")\n",
        "    # return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        rgb_sequence, depth_sequence, labels = batch\n",
        "        print(f\"RGB shape: {rgb_sequence.shape}\")\n",
        "        print(f\"Depth shape: {depth_sequence.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "        logits = self(rgb_sequence, depth_sequence)\n",
        "        print(f\"Logits shape: {logits.shape if logits is not None else None}\")\n",
        "\n",
        "        if logits is None:\n",
        "            print(\"Logits is None! Check the forward method.\")\n",
        "            return None\n",
        "\n",
        "        # Assuming you've added a classifier layer to handle the output from the forward method\n",
        "        # If not, you might need to add: self.classifier = nn.Linear(vit_output_dim, num_classes)\n",
        "        logits = self.classifier(\n",
        "            logits[:, -1]\n",
        "        )  # Using the last frame's features for classification\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        # Log metrics\n",
        "        self.log(\n",
        "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            \"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        rgb_sequence, depth_sequence, labels = batch\n",
        "        logits = self(rgb_sequence, depth_sequence)\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        # Log metrics\n",
        "        self.log(\n",
        "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            \"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.config[\"trainer\"][\"optimizer\"][\"weight_decay\"],\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            factor=self.config[\"trainer\"][\"scheduler\"][\"scheduler_factor\"],\n",
        "            patience=self.config[\"trainer\"][\"scheduler\"][\"scheduler_patience\"],\n",
        "            min_lr=self.config[\"trainer\"][\"scheduler\"][\"scheduler_min_lr\"],\n",
        "            verbose=True,\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"val_loss\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "\n",
        "config = load_config(\"configs/dummy.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classes(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        classes = [row[0] for row in reader]\n",
        "    return classes\n",
        "\n",
        "\n",
        "class_names = load_classes(config[\"dataset\"][\"classes_filepath\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    rgb_sequences, depth_sequences, labels = zip(*batch)\n",
        "\n",
        "    # pad the sequences to the maximum length within the batch\n",
        "    rgb_padded = pad_sequence(rgb_sequences, batch_first=True)\n",
        "    depth_padded = pad_sequence(depth_sequences, batch_first=True)\n",
        "\n",
        "    # get the maximum label length within the batch\n",
        "    max_label_length = max(len(label) for label in labels)\n",
        "\n",
        "    # pad the labels to the maximum length\n",
        "    padded_labels = []\n",
        "    for label in labels:\n",
        "        padded_label = label.tolist() + [-1] * (max_label_length - len(label))\n",
        "        padded_labels.append(torch.tensor(padded_label))\n",
        "\n",
        "    labels = torch.stack(padded_labels)\n",
        "\n",
        "    return rgb_padded, depth_padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = Args(config)\n",
        "\n",
        "# set up the datasets and dataloaders\n",
        "train_dataset = GSL_SI(config, args, mode=\"train\", classes=class_names)\n",
        "val_dataset = GSL_SI(config, args, mode=\"validation\", classes=class_names)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"dataset\"][\"train\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"train\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"train\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"dataset\"][\"validation\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"validation\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"validation\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialise model\n",
        "model = GSLDualViT(num_classes=config[\"dataset\"][\"classes\"], config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trainer setup and initialization\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config[\"trainer\"][\"epochs\"],\n",
        "    # accelerator='cpu' if config['trainer']['cuda'] else 'cpu', # uncomment this line to use CPU, and comment the next line\n",
        "    accelerator=\"gpu\" if config[\"trainer\"][\"cuda\"] else \"cpu\",\n",
        "    devices=1,\n",
        "    precision=\"16-mixed\",\n",
        "    accumulate_grad_batches=4,\n",
        "    num_sanity_val_steps=0,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(\n",
        "            dirpath=\"checkpoints\",\n",
        "            filename=\"gsl-dualvit-{epoch:02d}-{val_loss:.2f}\",\n",
        "            save_top_k=3,\n",
        "            monitor=\"val_loss\",\n",
        "            mode=\"min\",\n",
        "        ),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
        "    ],\n",
        "    enable_progress_bar=True,\n",
        "    log_every_n_steps=1,\n",
        "    logger=TensorBoardLogger(\"logs/\", name=config[\"trainer\"][\"logger\"]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get available memory info\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# free up gpu memory before training\n",
        "def free_gpu_cache():\n",
        "\n",
        "\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    cuda.select_device(0)\n",
        "\n",
        "\n",
        "    cuda.close()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "\n",
        "free_gpu_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# start training\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load the best model\n",
        "# best_model_path = checkpoint_callback.best_model_path\n",
        "# best_model = GSLDualViT.load_from_checkpoint(best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Evaluate on test set\n",
        "# test_dataset = GSLRGBDDataset(root_dir='path/to/GSL/dataset', split='test', transform=val_transform)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "# trainer.test(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Function to perform predictions\n",
        "# def perform_predictions(model, test_loader, num_samples=10):\n",
        "#     model.eval()\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     rgb_images = []\n",
        "#     depth_images = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_loader:\n",
        "#             rgb_seq, depth_seq, y = batch\n",
        "#             logits = model(rgb_seq, depth_seq)\n",
        "#             preds = logits.argmax(dim=-1)\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "#             all_labels.extend(y.cpu().numpy())\n",
        "#             rgb_images.extend(rgb_seq[:, 0].cpu().numpy())  # Take first frame of each sequence\n",
        "#             depth_images.extend(depth_seq[:, 0].cpu().numpy())\n",
        "\n",
        "#             if len(all_preds) >= num_samples:\n",
        "#                 break\n",
        "\n",
        "#     return rgb_images[:num_samples], depth_images[:num_samples], all_preds[:num_samples], all_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Visualize predictions\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# rgb_images, depth_images, preds, labels = perform_predictions(best_model, test_loader)\n",
        "\n",
        "# fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "# axes = axes.ravel()\n",
        "\n",
        "# for i, (rgb_img, depth_img, pred, label) in enumerate(zip(rgb_images, depth_images, preds, labels)):\n",
        "#     rgb_img = np.transpose(rgb_img, (1, 2, 0))\n",
        "#     rgb_img = (rgb_img * image_processor.image_std) + image_processor.image_mean\n",
        "#     rgb_img = np.clip(rgb_img, 0, 1)\n",
        "\n",
        "#     depth_img = np.squeeze(depth_img)\n",
        "\n",
        "#     axes[i].imshow(rgb_img)\n",
        "#     axes[i].imshow(depth_img, cmap='gray', alpha=0.5)\n",
        "#     axes[i].set_title(f'Pred: {test_dataset.classes[pred]}\\nTrue: {test_dataset.classes[label]}')\n",
        "#     axes[i].axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L8-vzGqedATR",
        "Mo8AJ5pXdUys",
        "1ae0s8Lav7Kw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
