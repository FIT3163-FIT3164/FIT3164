{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEt2gXhRcdm0"
      },
      "source": [
        "# FIT3164 - MDS08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMxCv1acsSW"
      },
      "source": [
        "## RUN: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bQfLjmLXU1Ej"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uiSiLYGZU4K1"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /root/.config/Google/DriveFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQaIuHdFI9x3",
        "outputId": "5ab1388f-69a1-43cf-c639-de5830071e4a"
      },
      "outputs": [],
      "source": [
        "# !python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjUXTuSkgf46",
        "outputId": "5893e30a-061d-48ee-a84f-b578bc562428"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install --ignore-installed tensorflow\n",
        "# !pip install --ignore-installed opencv-python\n",
        "# !pip install --ignore-installed pyrealsense2\n",
        "# !pip install --ignore-installed nltk\n",
        "# !pip install --ignore-installed streamlit\n",
        "# !pip install --ignore-installed keras\n",
        "# !pip install --ignore-installed flask\n",
        "# !pip install --ignore-installed flask-cors\n",
        "# !pip install --ignore-installed flask-assets\n",
        "# !pip install --ignore-installed flask-sslify\n",
        "# !pip install --ignore-installed flask-htmlmin\n",
        "# !pip install --ignore-installed sign-language-datasets\n",
        "# !pip install --ignore-installed gunicorn\n",
        "# !pip install --ignore-installed pillow\n",
        "# !pip install --ignore-installed mediapipe\n",
        "# !pip install git+https://github.com/faw01/datasets.git\n",
        "# !pip install mediapipe\n",
        "# !pip install git+https://github.com/dvolgyes/zenodo_get"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-vzGqedATR"
      },
      "source": [
        "## RUN: Test Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xciC5Yw_JBIy",
        "outputId": "45abae83-7e38-4011-b32e-3c40916e6998"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import keras\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n",
        "# print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgRq12NNJCjb",
        "outputId": "1e4f2fd4-f1eb-4f8a-8abf-428f1c806f40"
      },
      "outputs": [],
      "source": [
        "# physical_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "# print(\"Num GPUs:\", len(physical_devices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo8AJ5pXdUys"
      },
      "source": [
        "## DO NOT RUN: Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CL6si9MMwi-U"
      },
      "outputs": [],
      "source": [
        "# download_script = \"\"\"\n",
        "# #!/bin/bash\n",
        "\n",
        "# if [ $# -ne 2 ]; then\n",
        "#     echo \"Usage: $0 <file_url> <destination_path>\"\n",
        "#     exit 1\n",
        "# fi\n",
        "\n",
        "# file_url=$1\n",
        "# destination_path=$2\n",
        "\n",
        "# confirmation_page=$(curl -s -L \"$file_url\")\n",
        "\n",
        "# file_id=$(echo \"$confirmation_page\" | grep -oE \"name=\\\"id\\\" value=\\\"[^\\\"]+\" | sed 's/name=\"id\" value=\"//')\n",
        "# file_confirm=$(echo \"$confirmation_page\" | grep -oE \"name=\\\"confirm\\\" value=\\\"[^\\\"]+\" | sed 's/name=\"confirm\" value=\"//')\n",
        "# file_uuid=$(echo \"$confirmation_page\" | grep -oE \"name=\\\"uuid\\\" value=\\\"[^\\\"]+\" | sed 's/name=\"uuid\" value=\"//')\n",
        "\n",
        "# download_url=\"https://drive.usercontent.google.com/download?id=$file_id&export=download&confirm=$file_confirm&uuid=$file_uuid\"\n",
        "\n",
        "# curl -L -o \"$destination_path\" \"$download_url\"\n",
        "# \"\"\"\n",
        "\n",
        "# with open('download_script.sh', 'w') as file:\n",
        "#   file.write(download_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KXMWzcq4saGj"
      },
      "outputs": [],
      "source": [
        "# download_how2sign = \"\"\"\n",
        "# #!/bin/bash\n",
        "# #\n",
        "# # This script will create the folder structure and download the How2Sign dataset.\n",
        "# # For any questions about the following instructions or the data please contact: amanda.duarte[at]upc.edu\n",
        "# #\n",
        "# # To use this script, first choose the modalities that you would like to download and pass it as an argument to the command.\n",
        "# # For example, to download the \"rgb_front_videos\", the \"rgb_side_videos\" and the \"english_translation_re-aligned\" you can use the following command:\n",
        "# #\n",
        "# # ./download_how2sign.sh rgb_front_videos rgb_side_videos english_translation_re-aligned\n",
        "# #\n",
        "# # The names of the modalities avaliable for download can be found at the botton of this document\n",
        "# ################################################################################\n",
        "\n",
        "# # Provide at least one argument to script\n",
        "# if [ $# -lt 1 ]; then\n",
        "#     echo \"USAGE: $0 <argument1> <argument2> ...\"\n",
        "#     exit\n",
        "# fi\n",
        "\n",
        "# echo \"Downloading the How2Sign dataset\"\n",
        "\n",
        "# #############################################\n",
        "# # Create folder structure and download data #\n",
        "# #############################################\n",
        "\n",
        "# DOWNLOAD_SCRIPT=\"./download_script.sh\"\n",
        "\n",
        "\n",
        "# check_quota_exceeded() {\n",
        "#     # Use a timeout to prevent long grep operations on large files\n",
        "#     if timeout 10 grep -q \"Google Drive - Quota exceeded\" \"$1\"; then\n",
        "#         echo \"Quota exceeded error detected in $1.\"\n",
        "#         return 1 # Indicates error\n",
        "#     elif [ $? -eq 124 ]; then\n",
        "#         return 0 # Assume no error if timeout occurs\n",
        "#     fi\n",
        "#     return 0 # No error\n",
        "# }\n",
        "\n",
        "# # Download file if not exists or if error detected\n",
        "# download_file() {\n",
        "#     local url=$1\n",
        "#     local path=$2\n",
        "\n",
        "#     # Loop until the file is downloaded without errors\n",
        "#     while true; do\n",
        "#         # Check if the file exists\n",
        "#         if [ -f \"$path\" ]; then\n",
        "#             echo \"File $path already exists, checking content...\"\n",
        "#             # Check for Google Drive quota exceeded error\n",
        "#             check_quota_exceeded \"$path\"\n",
        "#             quota_exceeded=$?\n",
        "#             if [ $quota_exceeded -eq 0 ]; then\n",
        "#                 echo \"File $path is free of errors.\"\n",
        "#                 break # Exit loop if no error\n",
        "#             else\n",
        "#                 echo \"Redownloading $path due to error detection.\"\n",
        "#             fi\n",
        "#         else\n",
        "#             echo \"File $path does not exist, initiating download...\"\n",
        "#         fi\n",
        "\n",
        "#         # If file doesn't exist or error detected, download\n",
        "#         echo \"Downloading $path...\"\n",
        "#         sh $DOWNLOAD_SCRIPT \"$url\" \"$path\"\n",
        "\n",
        "#         # Check again if newly downloaded file has error\n",
        "#         check_quota_exceeded \"$path\"\n",
        "#         quota_exceeded=$?\n",
        "#         if [ $quota_exceeded -eq 0 ]; then\n",
        "#             echo \"File $path downloaded successfully and verified.\"\n",
        "#             break # Exit loop if file is correctly downloaded\n",
        "#         else\n",
        "#             echo \"Error detected post-download, retrying...\"\n",
        "#             sleep 1800 # Optional: Sleep to prevent hammering the server too quickly\n",
        "#         fi\n",
        "#     done\n",
        "# }\n",
        "\n",
        "\n",
        "# #------------------------- Green Screen RGB videos - Frontal View -------------------------#\n",
        "# rgb_front_videos() {\n",
        "#     mkdir -p \"./How2Sign/video_level/train/rgb_front\"\n",
        "#     mkdir -p \"./How2Sign/video_level/val/rgb_front\"\n",
        "#     mkdir -p \"./How2Sign/video_level/test/rgb_front\"\n",
        "\n",
        "#     echo \"***** Downloading Green Screen RGB videos (Frontal View)... You can go get a coffee, this might take a while!*****\"\n",
        "\n",
        "#     ## Train\n",
        "#     ### train_raw_videos.z01\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1xWlMM2O3Gbp_8LK5FefoH0TVEmae6jIf' 'train_raw_videos.z01'\n",
        "\n",
        "# \t### train_raw_videos.z02\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1krtYdpK_LQFgEUCnHxoYAW7EyhLMLWq0' 'train_raw_videos.z02'\n",
        "\n",
        "# \t### train_raw_videos.z03\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1fXpWRNFhpuVm3ym7lT9vF_bnDjHkvP_K' 'train_raw_videos.z03'\n",
        "\n",
        "# \t### train_raw_videos.z04\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1IFetFt4AzsxNCMZ0VVpX7YRgFAm58X48' 'train_raw_videos.z04'\n",
        "\n",
        "# \t### train_raw_videos.z05\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1ZHuuun6Ae-AOLBns3LmuH7w8C9YCB4gH' 'train_raw_videos.z05'\n",
        "\n",
        "# \t### train_raw_videos.z06\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1FQQIPblk-oLH_vu7h2tDO0oJaZ3xkp5N' 'train_raw_videos.z06'\n",
        "\n",
        "# \t### train_raw_videos.z07\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=19XNgERcolGAMPPgX-Gx_GebSTx3W4o0r' 'train_raw_videos.z07'\n",
        "\n",
        "# \t### train_raw_videos.z08\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1YN-SA9uzrogEdKeT6UdQUIcuGEyYJILg' 'train_raw_videos.z08'\n",
        "\n",
        "# \t### train_raw_videos.z09\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1SZQ2GzPLCkRqvsImAjULAPBiuAKi9DE9' 'train_raw_videos.z09'\n",
        "\n",
        "# \t### train_raw_videos.zip\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1Xe1T5okJiopMXUiH3sc0mdCWNDYSBopd' 'train_raw_videos.zip'\n",
        "\n",
        "#     ## Val\n",
        "#     ### val_raw_videos.zip\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1fCkyuKSsc7gauljuL9sx_jBomf3N6i0g' 'val_raw_videos.zip'\n",
        "\n",
        "#     ## Test\n",
        "#     ### test_raw_videos.zip\n",
        "#     download_file 'https://docs.google.com/uc?export=download&id=1z0i6BBGHQ12ChY63hZH56QnczvQ0JfTb' 'test_raw_videos.zip'\n",
        "\n",
        "#     # Merge all train zip files\n",
        "#     echo \"***** Preparing the downloaded files... this might take some time! *****\"\n",
        "#     cat train_raw_videos.z* > train_raw_videos_all.zip\n",
        "\n",
        "#     unzip train_raw_videos_all.zip -d ./How2Sign/video_level/train/rgb_front && rm -rf train_raw_videos_all.zip\n",
        "#     unzip val_raw_videos.zip   -d ./How2Sign/video_level/val/rgb_front && rm -rf val_raw_videos.zip\n",
        "#     unzip test_raw_videos.zip  -d ./How2Sign/video_level/test/rgb_front && rm -rf test_raw_videos.zip\n",
        "# }\n",
        "\n",
        "# #------------------------- Green Screen RGB videos - Side View -------------------------#\n",
        "# rgb_side_videos()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/video_level/train/rgb_side\"\n",
        "# \tmkdir -p \"./How2Sign/video_level/val/rgb_side\"\n",
        "# \tmkdir -p \"./How2Sign/video_level/test/rgb_side\"\n",
        "\n",
        "# \techo \"***** Downloading Green Screen RGB videos (Side View)... This might take a while! *****\"\n",
        "\n",
        "# \t## Train\n",
        "# \t### train_side_raw_videos.z01\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1Rmf6LfNWn6lWkAz6Iuj5pMOI2I5p4j1U' 'train_side_raw_videos.z01'\n",
        "\n",
        "# \t### train_side_raw_videos.z02\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1FytIYIRYrBgAeNWIAhO5vnI2mYOvYC9i' 'train_side_raw_videos.z02'\n",
        "\n",
        "# \t### train_side_raw_videos.z03\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1kC24jgNgjYYiIYhCRE-gGR28H_2xBBbP' 'train_side_raw_videos.z03'\n",
        "\n",
        "# \t### train_side_raw_videos.z04\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1JunkM-ImFYao_MwDW9zeqe-6Th6rOLhR' 'train_side_raw_videos.z04'\n",
        "\n",
        "# \t### train_side_raw_videos.z05\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1-vMckelz9fy4GVNYXRCcy7cJ12X4P3KZ' 'train_side_raw_videos.z05'\n",
        "\n",
        "# \t### train_side_raw_videos.z06\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1uV413eKsihkNzquN2bwtIQG-OZZMz6sh' 'train_side_raw_videos.z06'\n",
        "\n",
        "# \t### train_side_raw_videos.z07\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1sU8xrneFJHBzT_PFz4iRPqI8A7HGilhW' 'train_side_raw_videos.z07'\n",
        "\n",
        "# \t### train_side_raw_videos.z08\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1RPLxeZ54uSZUJSXdPFhXOgeIXziOwTW9' 'train_side_raw_videos.z08'\n",
        "\n",
        "# \t### train_side_raw_videos.z09\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1tClhr98PszBvFpo9ELKuhbTZZgTGGQqh' 'train_side_raw_videos.z09'\n",
        "\n",
        "# \t### train_side_raw_videos.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=10xrXWgH7iW3E6sgJZDPRwlIhIaDLfHQm' 'train_side_raw_videos.zip'\n",
        "\n",
        "# \t## Val\n",
        "# \t### val_rgb_side_raw_videos.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1Z2H96JT68o7eTChEXPI9z3xyx7zUJPl5' 'val_rgb_side_raw_videos.zip'\n",
        "\n",
        "# \t## Test\n",
        "# \t### test_rgb_side_raw_videos.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1tCQ8KIuuiirXHsh29w0XAMNB3HLIGqgA' 'test_rgb_side_raw_videos.zip'\n",
        "\n",
        "# \t# Merge all train zip files\n",
        "# \techo \"***** Preparing the downloaded files... this might take some time! *****\"\n",
        "# \tcat train_side_raw_videos.z* > train_side_raw_videos.zip\n",
        "\n",
        "# \tunzip train_side_raw_videos.zip -d ./How2Sign/video_level/train/rgb_side && rm -rf train_side_raw_videos.zip\n",
        "# \tunzip val_rgb_side_raw_videos.zip   -d ./How2Sign/video_level/val/rgb_side && rm -rf val_rgb_side_raw_videos.zip\n",
        "# \tunzip test_rgb_side_raw_videos.zip  -d ./How2Sign/video_level/test/rgb_side && rm -rf test_rgb_side_raw_videos.zip\n",
        "# }\n",
        "\n",
        "# #------------------------- Green Screen RGB clips -- Frontal view -------------------------#\n",
        "# rgb_front_clips()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/train/rgb_front\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/val/rgb_front\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/test/rgb_front\"\n",
        "\n",
        "# \techo \"***** Downloading and preparing the Green Screen RGB clips (Frontal view) videos *****\"\n",
        "\n",
        "# \t## Train\n",
        "# \t### train_rgb_front_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1VX7n0jjW0pW3GEdgOks3z8nqE6iI6EnW' 'train_rgb_front_clips.zip'\n",
        "\n",
        "# \t## Val\n",
        "# \t### val_rgb_front_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1DhLH8tIBn9HsTzUJUfsEOGcP4l9EvOiO' 'val_rgb_front_clips.zip'\n",
        "\n",
        "\n",
        "# \t## Test\n",
        "# \t### test_rgb_front_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1qTIXFsu8M55HrCiaGv7vZ7GkdB3ubjaG' 'test_rgb_front_clips.zip'\n",
        "\n",
        "# \tunzip train_rgb_front_clips.zip -d ./How2Sign/sentence_level/train/rgb_front && rm -rf train_rgb_front_clips.zip\n",
        "# \tunzip val_rgb_front_clips.zip   -d ./How2Sign/sentence_level/val/rgb_front && rm -rf val_rgb_front_clips.zip\n",
        "# \tunzip test_rgb_front_clips.zip  -d ./How2Sign/sentence_level/test/rgb_front && rm -rf test_rgb_front_clips.zip\n",
        "# }\n",
        "\n",
        "# #-------------------------  Green Screen RGB clips -- Side view -------------------------#\n",
        "# rgb_side_clips()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/train/rgb_side\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/val/rgb_side\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/test/rgb_side\"\n",
        "\n",
        "# \techo \"***** Downloading and preparing the Green Screen RGB clips (Side view) videos *****\"\n",
        "\n",
        "# \t## Train\n",
        "# \t### train_rgb_side_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1oiw861NGp4CKKFO3iuHGSCgTyQ-DXHW7' 'train_rgb_side_clips.zip'\n",
        "\n",
        "# \t## Val\n",
        "# \t### val_rgb_side_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1mxL7kJPNUzJ6zoaqJyxF1Krnjo4F-eQG' 'val_rgb_side_clips.zip'\n",
        "\n",
        "# \t## Test\n",
        "# \t### test_rgb_side_clips.zip\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1j9v9P7UdMJ0_FVWg8H95cqx4DMSsrdbH' 'test_rgb_side_clips.zip'\n",
        "\n",
        "\n",
        "# \tunzip train_rgb_side_clips.zip -d ./How2Sign/sentence_level/train/rgb_side && rm -rf train_rgb_side_clips.zip\n",
        "# \tunzip val_rgb_side_clips.zip   -d ./How2Sign/sentence_level/val/rgb_side && rm -rf val_rgb_side_clips.zip\n",
        "# \tunzip test_rgb_side_clips.zip  -d ./How2Sign/sentence_level/test/rgb_side && rm -rf test_rgb_side_clips.zip\n",
        "# }\n",
        "\n",
        "# #------------------------- B-F-H 2D Keypoints clips -- Frontal view -------------------------#\n",
        "# rgb_front_2D_keypoints()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/train/rgb_front/features\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/val/rgb_front/features\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/test/rgb_front/features\"\n",
        "\n",
        "# \techo \"***** Downloading B-F-H 2D Keypoints clips (Frontal view) files... This might take a while! *****\"\n",
        "# \t## Train\n",
        "# \t### train_2D_keypoints.tar.gz\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1TBX7hLraMiiLucknM1mhblNVomO9-Y0r' 'train_2D_keypoints.tar.gz'\n",
        "\n",
        "# \t## Val\n",
        "# \t### val_2D_keypoints.tar.gz\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1JmEsU0GYUD5iVdefMOZpeWa_iYnmK_7w' 'val_2D_keypoints.tar.gz'\n",
        "\n",
        "# \t## Test\n",
        "# \t### test_2D_keypoints.tar.gz\n",
        "# \tdownload_file 'https://docs.google.com/uc?export=download&id=1g8tzzW5BNPzHXlamuMQOvdwlHRa-29Vp' 'test_2D_keypoints.tar.gz'\n",
        "\n",
        "# \techo \"***** Preparing the downloaded files... this might take some time! *****\"\n",
        "# \ttar -xf train_2D_keypoints.tar.gz -C ./How2Sign/sentence_level/train/rgb_front/features && rm -rf train_2D_keypoints.tar.gz\n",
        "# \ttar -xf val_2D_keypoints.tar.gz   -C ./How2Sign/sentence_level/val/rgb_front/features && rm -rf val_2D_keypoints.tar.gz\n",
        "# \ttar -xf test_2D_keypoints.tar.gz  -C ./How2Sign/sentence_level/test/rgb_front/features && rm -rf test_2D_keypoints.tar.gz\n",
        "# }\n",
        "\n",
        "# # # B-F-H 2D Keypoints clips -- Side view\n",
        "# # rgb_side_2D_keypoints()\n",
        "# # {\n",
        "# # \techo \"Creating B-F-H 2D Keypoints clips -- Side view folders\"\n",
        "# # \tmkdir -p \"./How2Sign/sentence_level/train/rgb_side/features/openpose_output\"\n",
        "# # \tmkdir -p \"./How2Sign/sentence_level/val/rgb_side/features/openpose_output\"\n",
        "# # \tmkdir -p \"./How2Sign/sentence_level/test/rgb_side/features/openpose_output\"\n",
        "\n",
        "# # \tunzip train_rgb_side_2D_keypoints.zip -d ./How2Sign/sentence_level/train/rgb_side/features\n",
        "# # \tunzip val_rgb_side_2D_keypoints.zip   -d ./How2Sign/sentence_level/val/rgb_side/features\n",
        "# # \tunzip test_rgb_side_2D_keypoints.zip  -d ./How2Sign/sentence_level/test/rgb_side/features\n",
        "# # }\n",
        "\n",
        "# #------------------------- English Translation -------------------------#\n",
        "# english_translation()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/train/text/en/raw_text\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/val/text/en/raw_text\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/test/text/en/raw_text\"\n",
        "\n",
        "# \techo \"***** Downloading and preparing the English Translation text files *****\"\n",
        "# \t## Train\n",
        "# \t### how2sign_train.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1lq7ksWeD3FzaIwowRbe_BvCmSmOG12-f' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1lq7ksWeD3FzaIwowRbe_BvCmSmOG12-f\" -O  how2sign_train.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \t## Val\n",
        "# \t### how2sign_val.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aBQUClTlZB504JtDISJ0DJlbuYUZCGu3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1aBQUClTlZB504JtDISJ0DJlbuYUZCGu3\" -O  how2sign_val.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \t## Test\n",
        "# \t### how2sign_test.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ScxYnEjILZMn22qKjQj8Wyr_F0nha7kG' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ScxYnEjILZMn22qKjQj8Wyr_F0nha7kG\" -O  how2sign_test.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \tmv how2sign_train.csv How2Sign/sentence_level/train/text/en/raw_text\n",
        "# \tmv how2sign_val.csv How2Sign/sentence_level/val/text/en/raw_text\n",
        "# \tmv how2sign_test.csv How2Sign/sentence_level/test/text/en/raw_text\n",
        "# }\n",
        "\n",
        "# #------------------------- English Translation re-aligned -------------------------#\n",
        "# english_translation_re-aligned()\n",
        "# {\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/train/text/en/raw_text/re_aligned\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/val/text/en/raw_text/re_aligned\"\n",
        "# \tmkdir -p \"./How2Sign/sentence_level/test/text/en/raw_text/re_aligned\"\n",
        "\n",
        "# \techo \"***** Downloading and preparing the re-aligned English Translation text files *****\"\n",
        "# \t## Train\n",
        "# \t### how2sign_realigned_train.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dUHSoefk9OxKJnHrHPX--I4tpm9QD0ok' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1dUHSoefk9OxKJnHrHPX--I4tpm9QD0ok\" -O  how2sign_realigned_train.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \t## Val\n",
        "# \t### how2sign_realigned_val.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Vpag7VPfdTCCJSao8Pz14rlPfekRMggI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Vpag7VPfdTCCJSao8Pz14rlPfekRMggI\" -O  how2sign_realigned_val.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \t## Test\n",
        "# \t### how2sign_realigned_test.csv\n",
        "# \twget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1AgwBZW26kFHS4CWNMQTCMPGkBPkH3qCu' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1AgwBZW26kFHS4CWNMQTCMPGkBPkH3qCu\" -O  how2sign_realigned_test.csv && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# \tmv how2sign_realigned_train.csv How2Sign/sentence_level/train/text/en/raw_text/re_aligned\n",
        "# \tmv how2sign_realigned_val.csv How2Sign/sentence_level/val/text/en/raw_text/re_aligned\n",
        "# \tmv how2sign_realigned_test.csv How2Sign/sentence_level/test/text/en/raw_text/re_aligned\n",
        "# }\n",
        "\n",
        "# ## TODO\n",
        "# # Gloss annotations\n",
        "# # Panoptic Studio data\n",
        "\n",
        "# # Modalities avaliable for download\n",
        "# for ARG in \"$@\"\n",
        "# do\n",
        "# \tshift\n",
        "# \tcase \"${ARG}\" in\n",
        "# \t\t\"rgb_front_videos\") \t\trgb_front_videos;;\n",
        "# \t\t\"rgb_side_videos\")\t\trgb_side_videos;;\n",
        "# \t\t\"rgb_front_clips\")\t\trgb_front_clips;;\n",
        "# \t\t\"rgb_side_clips\")\t\trgb_side_clips;;\n",
        "# \t\t\"rgb_front_2D_keypoints\")\trgb_front_2D_keypoints;;\n",
        "# \t\t# \"rgb_side_2D_keypoints\")\trgb_side_2D_keypoints;;\n",
        "# \t\t\"english_translation\")\tenglish_translation;;\n",
        "# \t\t\"english_translation_re-aligned\")\tenglish_translation_re-aligned;;\n",
        "# \t\t*)\t\t\t\techo \"${ARG}: Invalid argument given\";;\n",
        "# \tesac\n",
        "# \techo \"Thank you for downloading the How2Sign dataset. Please check the README file for information about the files you just downloaded and feel free to contact us if you have any questions.\"\n",
        "# done\n",
        "# #\n",
        "# ################################################################################\n",
        "# \"\"\"\n",
        "\n",
        "# with open('download_how2sign.sh', 'w') as file:\n",
        "#   file.write(download_how2sign)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IL2nPkbVsZlX"
      },
      "outputs": [],
      "source": [
        "# !bash ./download_how2sign.sh rgb_front_videos rgb_side_videos english_translation_re-aligned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vLzqHMY9LP-C"
      },
      "outputs": [],
      "source": [
        "# !zenodo_get 4756317"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ae0s8Lav7Kw"
      },
      "source": [
        "## DO NOT RUN: Unzip Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_r6fj0fCw6Js"
      },
      "outputs": [],
      "source": [
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/train_rgb_front_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/train_rgb_front_clips\n",
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/val_rgb_front_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/val_rgb_front_clips\n",
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/test_rgb_front_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_front_clips/test_rgb_front_clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FHBbfjJ4wIq0"
      },
      "outputs": [],
      "source": [
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/train_rgb_side_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/train_rgb_side_clips\n",
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/val_rgb_side_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/val_rgb_side_clips\n",
        "# !unzip -q /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/test_rgb_side_clips.zip -d /content/drive/MyDrive/FIT3164/How2Sign/rgb_side_clips/test_rgb_side_clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S7jVVspgQZAU"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/train_2D_keypoints && tar -xzf /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/train_2D_keypoints.tar.gz -C /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/train_2D_keypoints\n",
        "# !mkdir -p /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/val_2D_keypoints && tar -xzf /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/val_2D_keypoints.tar.gz -C /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/val_2D_keypoints\n",
        "# !mkdir -p /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/test_2D_keypoints && tar -xzf /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/test_2D_keypoints.tar.gz -C /content/drive/MyDrive/FIT3164/How2Sign/bfh_front_keypoints/test_2D_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Cqoy97auTC2A"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQyZ8H60dnId"
      },
      "source": [
        "## DO NOT RUN: Setup Tensorflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "DoJDNw-GdQHG",
        "outputId": "2592411e-1474-4ebe-b89f-fff11f4b819d"
      },
      "outputs": [],
      "source": [
        "# import tensorflow_datasets as tfds\n",
        "# import sign_language_datasets.datasets\n",
        "# from sign_language_datasets.datasets.config import SignDatasetConfig\n",
        "# import itertools\n",
        "\n",
        "# config = SignDatasetConfig(\n",
        "#     name=\"default\",\n",
        "#     version=\"2.0.0\",\n",
        "#     include_video=False,\n",
        "#     include_pose=None,\n",
        "#     fps=25,\n",
        "#     resolution=(360, 360)\n",
        "# )\n",
        "\n",
        "# gsl_data_dir = \"gsl\"\n",
        "# gsl_split_dir = \"GSL_split\"\n",
        "\n",
        "# builder = tfds.builder('gsl', config=config)\n",
        "\n",
        "# download_config = tfds.download.DownloadConfig(\n",
        "#     manual_dir=gsl_data_dir,\n",
        "#     download_mode=tfds.download.GenerateMode.REUSE_DATASET_IF_EXISTS,\n",
        "# )\n",
        "\n",
        "# builder.download_and_prepare(download_config=download_config)\n",
        "\n",
        "# dataset = builder.as_dataset(split='train', shuffle_files=True)\n",
        "\n",
        "# for example in dataset:\n",
        "#     id = example['id'].numpy().decode('utf-8')\n",
        "#     gloss = example['gloss'].numpy().decode('utf-8')\n",
        "#     video_path = example['video_path'].numpy().decode('utf-8')\n",
        "#     depth_path = example['depth_path'].numpy().decode('utf-8')\n",
        "    \n",
        "#     print(f\"ID: {id}, Gloss: {gloss}\")\n",
        "#     print(f\"Video path: {video_path}\")\n",
        "#     print(f\"Depth path: {depth_path}\")\n",
        "#     print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RUN: Fine-tune and Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\GitHub\\FIT3164\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from transformers import ViTImageProcessor, ViTModel, ViTConfig\n",
        "import tensorboard\n",
        "import tensorboardX\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "import glob\n",
        "import random\n",
        "from base.base_loader import BaseDataset\n",
        "from datasets.loader_utils import (\n",
        "    multi_label_to_index,\n",
        "    pad_video,\n",
        "    video_transforms,\n",
        "    sampling,\n",
        "    VideoRandomResizedCrop,\n",
        "    read_gsl_continuous,\n",
        "    gsl_context,\n",
        "    read_bounding_box\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.cuda.is_available = lambda : False\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "HUGGING_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
        "# HUGGING_MODEL_NAME = 'google/vit-large-patch16-224-in21k'\n",
        "# HUGGING_MODEL_NAME = 'google/vit-huge-patch14-224-in21k'\n",
        "\n",
        "# BATCH_SIZE = 32\n",
        "# NUM_WORKERS = 2\n",
        "# MAX_EPOCHS = 10\n",
        "# LEARNING_RATE = 3e-4\n",
        "# SEQUENCE_LENGTH = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_processor = ViTImageProcessor.from_pretrained(HUGGING_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self, config):\n",
        "        self.cwd = os.getcwd()  # or specify the correct current working directory\n",
        "        self.input_data = config['dataset']['input_data']\n",
        "        self.return_context = False  # or set to True if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats_path = 'gsl_cont_features/'\n",
        "train_prefix = \"train\"\n",
        "dev_prefix = \"dev\"\n",
        "validation_prefix = \"validation\"\n",
        "test_prefix = \"test\"\n",
        "train_filepath = \"files/GSL_continuous/gsl_split_SI_train.csv\"\n",
        "dev_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "validation_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "test_filepath = \"files/GSL_continuous/gsl_split_SI_test.csv\"\n",
        "\n",
        "\n",
        "class GSL_SI(BaseDataset):\n",
        "    def __init__(self, config, args, mode, classes):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            config:\n",
        "            args:\n",
        "            mode:\n",
        "            classes:\n",
        "        \"\"\"\n",
        "        super(GSL_SI, self).__init__(config, args, mode, classes)\n",
        "\n",
        "        self.config = config['dataset']\n",
        "        self.mode = mode\n",
        "        self.dim = tuple(self.config['dim'])\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.num_classes = self.config['classes']\n",
        "        self.seq_length = self.config[self.mode]['seq_length']\n",
        "        self.normalize = self.config['normalize']\n",
        "        self.padding = self.config['padding']\n",
        "        self.augmentation = self.config[self.mode]['augmentation']\n",
        "        self.return_context = args.return_context\n",
        "\n",
        "        self.rgb_data_path = os.path.join(self.config['input_data'], self.config['images_path'])\n",
        "        self.depth_data_path = os.path.join(self.config['input_data'], self.config['depth_path'])\n",
        "\n",
        "        if self.mode == train_prefix:\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(os.path.join(args.cwd, train_filepath))\n",
        "        elif self.mode == validation_prefix:\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(os.path.join(args.cwd, validation_filepath))\n",
        "        elif self.mode == test_prefix:\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(os.path.join(args.cwd, test_filepath))\n",
        "\n",
        "        print(f\"{len(self.list_IDs)} {self.mode} instances\")\n",
        "\n",
        "        self.bbox = read_bounding_box(os.path.join(args.cwd, 'files/GSL_continuous/bbox_for_gsl_continuous.txt'))\n",
        "\n",
        "        self.context = gsl_context(self.list_IDs, self.list_glosses)\n",
        "\n",
        "        if self.config['modality'] == 'full':\n",
        "            self.data_path = os.path.join(self.config['input_data'], self.config['images_path'])\n",
        "            self.get = self.video_loader\n",
        "        elif self.config['modality'] == 'features':\n",
        "            self.data_path = os.path.join(self.config['input_data'], self.config['features_path'])\n",
        "            self.get = self.feature_loader\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        rgb_x = self.load_video_sequence(path=self.list_IDs[index], img_type='jpg')\n",
        "        depth_x = self.load_video_sequence(path=self.list_IDs[index].replace('color', 'depth'), img_type='jpg')\n",
        "        y = multi_label_to_index(classes=self.classes, target_labels=self.list_glosses[index])\n",
        "        \n",
        "        print(f\"In __getitem__: RGB shape: {rgb_x.shape}\")\n",
        "        print(f\"In __getitem__: Depth shape: {depth_x.shape}\")\n",
        "        print(f\"In __getitem__: Label shape: {y.shape}\")\n",
        "        \n",
        "        return rgb_x, depth_x, y\n",
        "\n",
        "    def feature_loader(self, index):\n",
        "        folder_path = os.path.join(self.data_path, self.list_IDs[index])\n",
        "        # print(folder_path)\n",
        "\n",
        "        y = multi_label_to_index(classes=self.classes, target_labels=self.list_glosses[index])\n",
        "        if self.context[index] != None:\n",
        "\n",
        "            c = multi_label_to_index(classes=self.classes, target_labels=self.context[index])\n",
        "        else:\n",
        "            c = torch.tensor([0], dtype=torch.int)\n",
        "        x = torch.FloatTensor(np.load(folder_path + '.npy')).squeeze(0)\n",
        "        if self.return_context:\n",
        "            return x, [y, c]\n",
        "        return x, y\n",
        "\n",
        "    def video_loader(self, index):\n",
        "\n",
        "        x = self.load_video_sequence(path=self.list_IDs[index],\n",
        "                                     img_type='jpg')\n",
        "        y = multi_label_to_index(classes=self.classes, target_labels=self.list_glosses[index])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def load_video_sequence(self, path, img_type='jpg', is_depth=False):\n",
        "        data_path = self.depth_data_path if is_depth else self.rgb_data_path\n",
        "        images = sorted(glob.glob(os.path.join(data_path, path, ) + '/*' + img_type))\n",
        "\n",
        "        h_flip = False\n",
        "        img_sequence = []\n",
        "        # print(images)\n",
        "        if (len(images) < 1):\n",
        "            print(os.path.join(self.data_path, path))\n",
        "        bbox = self.bbox.get(path)\n",
        "\n",
        "        if (self.augmentation):\n",
        "            ## training set temporal  AUGMENTATION\n",
        "            temporal_augmentation = int((np.random.randint(80, 100) / 100.0) * len(images))\n",
        "            if (temporal_augmentation > 15):\n",
        "                images = sorted(random.sample(images, k=temporal_augmentation))\n",
        "            if (len(images) > self.seq_length):\n",
        "                # random frame sampling\n",
        "                images = sorted(random.sample(images, k=self.seq_length))\n",
        "\n",
        "        else:\n",
        "            # test uniform sampling\n",
        "            if (len(images) > self.seq_length):\n",
        "                images = sorted(sampling(images, self.seq_length))\n",
        "\n",
        "        i = np.random.randint(0, 30)\n",
        "        j = np.random.randint(0, 30)\n",
        "        brightness = 1 + random.uniform(-0.2, +0.2)\n",
        "        contrast = 1 + random.uniform(-0.2, +0.2)\n",
        "        hue = random.uniform(0, 1) / 10.0\n",
        "        # r_resize = ((112, 112))\n",
        "        r_resize = ((224, 224))\n",
        "        crop_or_bbox = random.uniform(0, 1) > 0.5\n",
        "        to_flip = random.uniform(0, 1) > 1\n",
        "        grayscale = random.uniform(0, 1) > 0.9\n",
        "        t1 = VideoRandomResizedCrop(self.dim[0], scale=(0.9, 1.0), ratio=(0.8, 1.2))\n",
        "        for img_path in images:\n",
        "\n",
        "            frame_o = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            crop_size = 120\n",
        "            ## CROP BOUNDING BOX\n",
        "            ## CROP BOUNDING BOX\n",
        "\n",
        "            frame1 = np.array(frame_o)\n",
        "\n",
        "            frame1 = frame1[:, crop_size:648 - crop_size]\n",
        "            frame = Image.fromarray(frame1)\n",
        "\n",
        "            if self.augmentation:\n",
        "\n",
        "                ## training set DATA AUGMENTATION\n",
        "\n",
        "                frame = frame.resize(r_resize)\n",
        "\n",
        "                img_tensor = video_transforms(img=frame, i=i, j=j, bright=brightness, cont=contrast, h=hue,\n",
        "                                            dim=self.dim,\n",
        "                                            resized_crop=t1,\n",
        "                                            augmentation=True,\n",
        "                                            normalize=self.normalize, crop=crop_or_bbox, to_flip=to_flip,\n",
        "                                            grayscale=grayscale)\n",
        "                img_sequence.append(img_tensor)\n",
        "            else:\n",
        "                # TEST set  NO DATA AUGMENTATION\n",
        "                if is_depth:\n",
        "                    # Resize depth frame to match RGB dimensions\n",
        "                    frame = frame.resize(self.dim)\n",
        "                else:\n",
        "                    frame = frame.resize(self.dim)\n",
        "\n",
        "                img_tensor = video_transforms(img=frame, i=i, j=j, bright=1, cont=1, h=0, dim=self.dim,\n",
        "                                            augmentation=False,\n",
        "                                            normalize=self.normalize)\n",
        "                img_sequence.append(img_tensor)\n",
        "        pad_len = self.seq_length - len(images)\n",
        "\n",
        "        X1 = torch.stack(img_sequence).float()\n",
        "\n",
        "        if (self.padding):\n",
        "            X1 = pad_video(X1, padding_size=pad_len, padding_type='zeros')\n",
        "        if (len(images) < 25):\n",
        "            X1 = pad_video(X1, padding_size=25 - len(images), padding_type='zeros')\n",
        "        return X1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GSLDualViT(pl.LightningModule):\n",
        "    def __init__(self, num_classes, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.class_names = class_names\n",
        "        self.num_classes = len(class_names)\n",
        "        # self.logger = logger\n",
        "        self.learning_rate = config['trainer']['optimizer']['lr']\n",
        "        self.sequence_length = config['dataset']['train']['seq_length']\n",
        "        \n",
        "        # RGB ViT\n",
        "        self.rgb_vit = ViTModel.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        self.rgb_vit.train()\n",
        "\n",
        "        for param in self.rgb_vit.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        # Depth ViT (initialize with same config but different input channels)\n",
        "        depth_config = ViTConfig.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        depth_config.num_channels = 3\n",
        "        self.depth_vit = ViTModel(depth_config)\n",
        "        \n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Linear(self.rgb_vit.config.hidden_size * 2, self.rgb_vit.config.hidden_size)\n",
        "        \n",
        "        # Temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.rgb_vit.config.hidden_size, \n",
        "            self.config['trainer']['model']['backbone']['rnn']['hidden_size'], \n",
        "            num_layers=self.config['trainer']['model']['backbone']['rnn']['num_layers'],\n",
        "            bidirectional=self.config['trainer']['model']['backbone']['rnn']['bidirectional'],\n",
        "            batch_first=True,\n",
        "            dropout=self.config['trainer']['model']['backbone']['rnn']['dropout']\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        lstm_output_size = self.config['trainer']['model']['backbone']['rnn']['hidden_size']\n",
        "        if self.config['trainer']['model']['backbone']['rnn']['bidirectional']:\n",
        "            lstm_output_size *= 2\n",
        "        self.classifier = nn.Linear(lstm_output_size, num_classes)\n",
        "    \n",
        "    def forward(self, rgb_sequence, depth_sequence=None):\n",
        "        print(f\"Forward pass input shapes: RGB {rgb_sequence.shape}, Depth {depth_sequence.shape if depth_sequence is not None else None}\")\n",
        "        batch_size, seq_len, channels, height, width = rgb_sequence.shape\n",
        "        fused_features = []\n",
        "        for i in range(seq_len):\n",
        "            print(f\"Processing frame {i}\")\n",
        "            try:\n",
        "                rgb_features = self.rgb_vit(rgb_sequence[:, i]).last_hidden_state[:, 0]\n",
        "                print(f\"RGB features shape: {rgb_features.shape}\")\n",
        "                \n",
        "                if depth_sequence is not None:\n",
        "                    depth_features = self.depth_vit(depth_sequence[:, i]).last_hidden_state[:, 0]\n",
        "                    print(f\"Depth features shape: {depth_features.shape}\")\n",
        "                    fused = self.fusion(torch.cat([rgb_features, depth_features], dim=-1))\n",
        "                else:\n",
        "                    fused = rgb_features\n",
        "                \n",
        "                print(f\"Fused features shape: {fused.shape}\")\n",
        "                fused_features.append(fused)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {i}: {str(e)}\")\n",
        "        \n",
        "        print(f\"Number of fused features: {len(fused_features)}\")\n",
        "        if len(fused_features) == 0:\n",
        "            print(\"fused_features is empty!\")\n",
        "            return None\n",
        "        \n",
        "        fused_sequence = torch.stack(fused_features, dim=1)\n",
        "        print(f\"Fused sequence shape: {fused_sequence.shape}\")\n",
        "        lstm_out, _ = self.lstm(fused_sequence)\n",
        "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
        "        output = self.classifier(lstm_out[:, -1])\n",
        "        print(f\"Final output shape: {output.shape}\")\n",
        "        return fused_sequence  # Changed from 'output' to 'fused_sequence'\n",
        "        \n",
        "    # def forward(self, rgb_sequence, depth_sequence=None):\n",
        "    #     batch_size, seq_len, channels, height, width = rgb_sequence.shape\n",
        "    #     print(f\"In forward: RGB shape: {rgb_sequence.shape}\")\n",
        "        \n",
        "        # Process each frame in the sequence\n",
        "        # fused_features = []\n",
        "        # for i in range(seq_len):\n",
        "        #     rgb_features = self.rgb_vit(rgb_sequence[:, i]).last_hidden_state[:, 0]  # Use CLS token\n",
        "            \n",
        "        #     if depth_sequence is not None:\n",
        "        #         print(f\"In forward: Depth shape: {depth_sequence.shape}\")\n",
        "        #         depth_sequence_resized = F.interpolate(depth_sequence[:, i], size=(height, width), mode='bilinear', align_corners=False)\n",
        "        #         depth_features = self.depth_vit(depth_sequence_resized).last_hidden_state[:, 0]  # Use CLS token\n",
        "        #         fused = self.fusion(torch.cat([rgb_features, depth_features], dim=-1))\n",
        "        #     else:\n",
        "        #         fused = rgb_features\n",
        "            \n",
        "        #     fused_features.append(fused)\n",
        "        \n",
        "        # fused_sequence = torch.stack(fused_features, dim=1)\n",
        "        \n",
        "        # Temporal modeling\n",
        "        # lstm_out, _ = self.lstm(fused_sequence)\n",
        "        \n",
        "        # Classification\n",
        "        # output = self.classifier(lstm_out[:, -1])  # Use last timestep for classification\n",
        "        # print(f\"In forward: Output shape: {output.shape}\")\n",
        "        # return output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        rgb_sequence, depth_sequence, labels = batch\n",
        "        print(f\"RGB shape: {rgb_sequence.shape}\")\n",
        "        print(f\"Depth shape: {depth_sequence.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "        logits = self(rgb_sequence, depth_sequence)\n",
        "        print(f\"Logits shape: {logits.shape if logits is not None else None}\")\n",
        "\n",
        "        if logits is None:\n",
        "            print(\"Logits is None! Check the forward method.\")\n",
        "            return None\n",
        "\n",
        "        # Assuming you've added a classifier layer to handle the output from the forward method\n",
        "        # If not, you might need to add: self.classifier = nn.Linear(vit_output_dim, num_classes)\n",
        "        logits = self.classifier(logits[:, -1])  # Using the last frame's features for classification\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "        \n",
        "        # Log metrics\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        rgb_sequence, depth_sequence, labels = batch\n",
        "        logits = self(rgb_sequence, depth_sequence)\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "        \n",
        "        # Log metrics\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(\n",
        "            self.parameters(), \n",
        "            lr=self.learning_rate, \n",
        "            weight_decay=self.config['trainer']['optimizer']['weight_decay']\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            factor=self.config['trainer']['scheduler']['scheduler_factor'],\n",
        "            patience=self.config['trainer']['scheduler']['scheduler_patience'],\n",
        "            min_lr=self.config['trainer']['scheduler']['scheduler_min_lr'],\n",
        "            verbose=True\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"val_loss\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import csv\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "config = load_config('configs/dummy.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classes(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        classes = [row[0] for row in reader]\n",
        "    return classes\n",
        "\n",
        "class_names = load_classes(config['dataset']['classes_filepath'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    rgb_sequences, depth_sequences, labels = zip(*batch)\n",
        "    \n",
        "    # Pad the sequences to the maximum length within the batch\n",
        "    rgb_padded = pad_sequence(rgb_sequences, batch_first=True)\n",
        "    depth_padded = pad_sequence(depth_sequences, batch_first=True)\n",
        "    \n",
        "    # Get the maximum label length within the batch\n",
        "    max_label_length = max(len(label) for label in labels)\n",
        "    \n",
        "    # Pad the labels to the maximum length\n",
        "    padded_labels = []\n",
        "    for label in labels:\n",
        "        padded_label = label.tolist() + [-1] * (max_label_length - len(label))\n",
        "        padded_labels.append(torch.tensor(padded_label))\n",
        "    \n",
        "    labels = torch.stack(padded_labels)\n",
        "    \n",
        "    return rgb_padded, depth_padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8821 train instances\n",
            "588 validation instances\n"
          ]
        }
      ],
      "source": [
        "args = Args(config)\n",
        "\n",
        "# Set up data loaders\n",
        "train_dataset = GSL_SI(config, args, mode='train', classes=class_names)\n",
        "val_dataset = GSL_SI(config, args, mode='validation', classes=class_names)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=config['dataset']['train']['batch_size'],\n",
        "    shuffle=config['dataset']['train']['shuffle'], \n",
        "    num_workers=config['dataset']['train']['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, \n",
        "    batch_size=config['dataset']['validation']['batch_size'],\n",
        "    shuffle=config['dataset']['validation']['shuffle'], \n",
        "    num_workers=config['dataset']['validation']['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GSLDualViT(num_classes=config['dataset']['classes'], config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "# Set up trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config['trainer']['epochs'],\n",
        "    # accelerator='cpu' if config['trainer']['cuda'] else 'cpu',\n",
        "    accelerator='gpu' if config['trainer']['cuda'] else 'cpu',\n",
        "    devices=1,\n",
        "    precision=\"16-mixed\",\n",
        "    accumulate_grad_batches=4,\n",
        "    num_sanity_val_steps=0,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(\n",
        "            dirpath='checkpoints',\n",
        "            filename='gsl-dualvit-{epoch:02d}-{val_loss:.2f}',\n",
        "            save_top_k=3,\n",
        "            monitor='val_loss',\n",
        "            mode='min'\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            mode='min'\n",
        "        )\n",
        "    ],\n",
        "    enable_progress_bar=True,\n",
        "    log_every_n_steps=1,\n",
        "    logger=TensorBoardLogger('logs/', name=config['trainer']['logger'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5355077632, 6442123264)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  1% |  1% |\n",
            "GPU Usage after emptying the cache\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  1% |  1% |\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()                             \n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "free_gpu_cache() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type     | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | rgb_vit    | ViTModel | 86.4 M | train\n",
            "1 | depth_vit  | ViTModel | 86.4 M | train\n",
            "2 | fusion     | Linear   | 1.2 M  | train\n",
            "3 | lstm       | LSTM     | 11.6 M | train\n",
            "4 | classifier | Linear   | 318 K  | train\n",
            "------------------------------------------------\n",
            "185 M     Trainable params\n",
            "0         Non-trainable params\n",
            "185 M     Total params\n",
            "743.314   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/552 [00:00<?, ?it/s] In __getitem__: RGB shape: torch.Size([157, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([172, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([6])\n",
            "In __getitem__: RGB shape: torch.Size([78, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([81, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([3])\n",
            "In __getitem__: RGB shape: torch.Size([83, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([82, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([4])\n",
            "In __getitem__: RGB shape: torch.Size([89, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([94, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([6])\n",
            "In __getitem__: RGB shape: torch.Size([114, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([130, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([5])\n",
            "In __getitem__: RGB shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([1])\n",
            "In __getitem__: RGB shape: torch.Size([174, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([169, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([5])\n",
            "In __getitem__: RGB shape: torch.Size([68, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([72, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([5])\n",
            "In __getitem__: RGB shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([1])\n",
            "In __getitem__: RGB shape: torch.Size([108, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([99, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([5])\n",
            "In __getitem__: RGB shape: torch.Size([111, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([108, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([5])\n",
            "In __getitem__: RGB shape: torch.Size([66, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([68, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([4])\n",
            "In __getitem__: RGB shape: torch.Size([141, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([138, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([7])\n",
            "In __getitem__: RGB shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([25, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([2])\n",
            "In __getitem__: RGB shape: torch.Size([41, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([42, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([2])\n",
            "In __getitem__: RGB shape: torch.Size([78, 3, 224, 224])\n",
            "In __getitem__: Depth shape: torch.Size([72, 3, 224, 224])\n",
            "In __getitem__: Label shape: torch.Size([4])\n",
            "RGB shape: torch.Size([16, 174, 3, 224, 224])\n",
            "Depth shape: torch.Size([16, 172, 3, 224, 224])\n",
            "Labels shape: torch.Size([16, 7])\n",
            "Forward pass input shapes: RGB torch.Size([16, 174, 3, 224, 224]), Depth torch.Size([16, 172, 3, 224, 224])\n",
            "Processing frame 0\n",
            "RGB features shape: torch.Size([16, 768])\n",
            "Depth features shape: torch.Size([16, 768])\n",
            "Fused features shape: torch.Size([16, 768])\n",
            "Processing frame 1\n",
            "RGB features shape: torch.Size([16, 768])\n",
            "Depth features shape: torch.Size([16, 768])\n",
            "Fused features shape: torch.Size([16, 768])\n",
            "Processing frame 2\n",
            "RGB features shape: torch.Size([16, 768])\n",
            "Depth features shape: torch.Size([16, 768])\n",
            "Fused features shape: torch.Size([16, 768])\n",
            "Processing frame 3\n",
            "Error processing frame 3: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 4\n",
            "Error processing frame 4: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 5\n",
            "Error processing frame 5: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 6\n",
            "Error processing frame 6: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 7\n",
            "Error processing frame 7: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 8\n",
            "Error processing frame 8: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 9\n",
            "Error processing frame 9: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 10\n",
            "Error processing frame 10: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 11\n",
            "Error processing frame 11: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 12\n",
            "Error processing frame 12: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
            "Processing frame 13\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model_path = checkpoint_callback.best_model_path\n",
        "best_model = GSLDualViT.load_from_checkpoint(best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_dataset = GSLRGBDDataset(root_dir='path/to/GSL/dataset', split='test', transform=val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "trainer.test(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to perform predictions\n",
        "def perform_predictions(model, test_loader, num_samples=10):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    rgb_images = []\n",
        "    depth_images = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            rgb_seq, depth_seq, y = batch\n",
        "            logits = model(rgb_seq, depth_seq)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            rgb_images.extend(rgb_seq[:, 0].cpu().numpy())  # Take first frame of each sequence\n",
        "            depth_images.extend(depth_seq[:, 0].cpu().numpy())\n",
        "            \n",
        "            if len(all_preds) >= num_samples:\n",
        "                break\n",
        "    \n",
        "    return rgb_images[:num_samples], depth_images[:num_samples], all_preds[:num_samples], all_labels[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rgb_images, depth_images, preds, labels = perform_predictions(best_model, test_loader)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (rgb_img, depth_img, pred, label) in enumerate(zip(rgb_images, depth_images, preds, labels)):\n",
        "    rgb_img = np.transpose(rgb_img, (1, 2, 0))\n",
        "    rgb_img = (rgb_img * image_processor.image_std) + image_processor.image_mean\n",
        "    rgb_img = np.clip(rgb_img, 0, 1)\n",
        "    \n",
        "    depth_img = np.squeeze(depth_img)\n",
        "    \n",
        "    axes[i].imshow(rgb_img)\n",
        "    axes[i].imshow(depth_img, cmap='gray', alpha=0.5)\n",
        "    axes[i].set_title(f'Pred: {test_dataset.classes[pred]}\\nTrue: {test_dataset.classes[label]}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "L8-vzGqedATR",
        "Mo8AJ5pXdUys",
        "1ae0s8Lav7Kw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
