{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEt2gXhRcdm0"
      },
      "source": [
        "# FIT3164 - MDS08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMxCv1acsSW"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbmj56OHY1nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1f8eae-f289-4903-a34d-fe7a3c40a7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoB3qXjgoyy5"
      },
      "outputs": [],
      "source": [
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVovmLeTV5TD"
      },
      "outputs": [],
      "source": [
        "# uncomment if using cpu training\n",
        "# !pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLmxiKrVoDby"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install pytorch-lightning tensorboardX omegaconf transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja73dKeHwXeL"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"drive/Shareddrives/FIT3164/model\")\n",
        "!chmod -R 755 base configs files FIT3164_GSL.ipynb logger logs models trainer utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4RvTkBdoDb1"
      },
      "source": [
        "## Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYcYdI4loDb1"
      },
      "outputs": [],
      "source": [
        "# all imports\n",
        "# from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "import cv2\n",
        "import logging\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import yaml\n",
        "import csv\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "from transformers import ViTImageProcessor, ViTModel, ViTConfig\n",
        "\n",
        "\n",
        "import tensorboard\n",
        "\n",
        "\n",
        "import tensorboardX\n",
        "\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "\n",
        "\n",
        "from base.base_loader import BaseDataset\n",
        "\n",
        "\n",
        "from datasets.loader_utils import (\n",
        "    multi_label_to_index,\n",
        "    pad_video,\n",
        "    video_transforms,\n",
        "    sampling,\n",
        "    VideoRandomResizedCrop,\n",
        "    read_gsl_continuous,\n",
        "    gsl_context,\n",
        "    read_bounding_box,\n",
        ")\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gc\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weQxZD15oDb2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7zeImW2oDb2"
      },
      "outputs": [],
      "source": [
        "# check if cuda is available\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# uncomment and run this cell to use cpu\n",
        "# torch.cuda.is_available = lambda : False\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# check if cuda is available (should say False if the above cell is run)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7ktFjrHoDb2"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0RBM3TGoDb2"
      },
      "outputs": [],
      "source": [
        "HUGGING_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
        "# HUGGING_MODEL_NAME = 'google/vit-large-patch16-224-in21k'\n",
        "# HUGGING_MODEL_NAME = 'google/vit-huge-patch14-224-in21k'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5z4BM_VoDb3"
      },
      "outputs": [],
      "source": [
        "image_processor = ViTImageProcessor.from_pretrained(HUGGING_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps-5BYjxoDb3"
      },
      "outputs": [],
      "source": [
        "# too much work to remove args from the code, so just create a dummy class and pass it to the functions\n",
        "class Args:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.cwd = os.getcwd()\n",
        "\n",
        "        self.input_data = config[\"dataset\"][\"input_data\"]\n",
        "\n",
        "        self.return_context = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats_path = \"gsl_cont_features/\"\n",
        "\n",
        "\n",
        "train_prefix = \"train\"\n",
        "\n",
        "\n",
        "dev_prefix = \"dev\"\n",
        "\n",
        "\n",
        "validation_prefix = \"validation\"\n",
        "\n",
        "\n",
        "test_prefix = \"test\"\n",
        "\n",
        "\n",
        "train_filepath = \"files/GSL_continuous/gsl_split_SI_train.csv\"\n",
        "\n",
        "\n",
        "dev_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "\n",
        "\n",
        "validation_filepath = \"files/GSL_continuous/gsl_split_SI_dev.csv\"\n",
        "\n",
        "\n",
        "test_filepath = \"files/GSL_continuous/gsl_split_SI_test.csv\""
      ],
      "metadata": {
        "id": "tjfQZOBGBqcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV1aFUMV1zoi"
      },
      "outputs": [],
      "source": [
        "class GSL_SI(BaseDataset):\n",
        "\n",
        "    def __init__(self, config, args, mode, classes):\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        Args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            config:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            args:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            mode:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            classes:\n",
        "        \"\"\"\n",
        "\n",
        "        super(GSL_SI, self).__init__(config, args, mode, classes)\n",
        "\n",
        "        self.config = config[\"dataset\"]\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dim = tuple(self.config[\"dim\"])\n",
        "        self.classes = classes\n",
        "\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        self.num_classes = self.config[\"classes\"]\n",
        "\n",
        "        self.seq_length = self.config[self.mode][\"seq_length\"]\n",
        "\n",
        "        self.normalize = self.config[\"normalize\"]\n",
        "\n",
        "        self.padding = self.config[\"padding\"]\n",
        "\n",
        "        self.augmentation = self.config[self.mode][\"augmentation\"]\n",
        "\n",
        "        self.return_context = args.return_context\n",
        "\n",
        "        self.rgb_data_path = os.path.join(\n",
        "            self.config[\"input_data\"], self.config[\"images_path\"]\n",
        "        )\n",
        "\n",
        "        self.depth_data_path = os.path.join(\n",
        "            self.config[\"input_data\"], self.config[\"depth_path\"]\n",
        "        )\n",
        "\n",
        "        if self.mode == train_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, train_filepath)\n",
        "            )\n",
        "\n",
        "        elif self.mode == validation_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, validation_filepath)\n",
        "            )\n",
        "\n",
        "        elif self.mode == test_prefix:\n",
        "\n",
        "            self.list_IDs, self.list_glosses = read_gsl_continuous(\n",
        "                os.path.join(args.cwd, test_filepath)\n",
        "            )\n",
        "\n",
        "        print(f\"{len(self.list_IDs)} {self.mode} instances\")\n",
        "\n",
        "        self.bbox = read_bounding_box(\n",
        "            os.path.join(args.cwd, \"files/GSL_continuous/bbox_for_gsl_continuous.txt\")\n",
        "        )\n",
        "\n",
        "        self.context = gsl_context(self.list_IDs, self.list_glosses)\n",
        "\n",
        "        if self.config[\"modality\"] == \"full\":\n",
        "\n",
        "            self.data_path = os.path.join(\n",
        "                self.config[\"input_data\"], self.config[\"images_path\"]\n",
        "            )\n",
        "\n",
        "            self.get = self.video_loader\n",
        "\n",
        "        elif self.config[\"modality\"] == \"features\":\n",
        "\n",
        "            self.data_path = os.path.join(\n",
        "                self.config[\"input_data\"], self.config[\"features_path\"]\n",
        "            )\n",
        "\n",
        "            self.get = self.feature_loader\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.list_IDs[index]\n",
        "\n",
        "        rgb_x = self.load_video_sequence(path=self.list_IDs[index], img_type=\"jpg\")\n",
        "\n",
        "        # depth_x = self.load_video_sequence(\n",
        "        #     path=self.list_IDs[index].replace(\"color\", \"depth\"), img_type=\"jpg\"\n",
        "        # )\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "        print(f\"  File path: {file_path}\")\n",
        "        print(f\"In __getitem__ (index {index}): RGB shape: {rgb_x.shape}\")\n",
        "        # print(f\"In __getitem__ (index {index}): Depth shape: {depth_x.shape}\")\n",
        "        print(f\"In __getitem__ (index {index}): Label shape: {y.shape}\")\n",
        "\n",
        "        # return rgb_x, depth_x, y\n",
        "        return rgb_x, y\n",
        "\n",
        "    def feature_loader(self, index):\n",
        "\n",
        "        folder_path = os.path.join(self.data_path, self.list_IDs[index])\n",
        "\n",
        "        # print(folder_path)\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "\n",
        "        if self.context[index] != None:\n",
        "\n",
        "            c = multi_label_to_index(\n",
        "                classes=self.classes, target_labels=self.context[index]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "\n",
        "            c = torch.tensor([0], dtype=torch.int)\n",
        "\n",
        "        x = torch.FloatTensor(np.load(folder_path + \".npy\")).squeeze(0)\n",
        "\n",
        "        if self.return_context:\n",
        "\n",
        "            return x, [y, c]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def video_loader(self, index):\n",
        "\n",
        "        x = self.load_video_sequence(path=self.list_IDs[index], img_type=\"jpg\")\n",
        "\n",
        "        y = multi_label_to_index(\n",
        "            classes=self.classes, target_labels=self.list_glosses[index]\n",
        "        )\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def load_video_sequence(self, path, img_type=\"jpg\", is_depth=False):\n",
        "\n",
        "        data_path = self.depth_data_path if is_depth else self.rgb_data_path\n",
        "\n",
        "        images = sorted(\n",
        "            glob.glob(\n",
        "                os.path.join(\n",
        "                    data_path,\n",
        "                    path,\n",
        "                )\n",
        "                + \"/*\"\n",
        "                + img_type\n",
        "            )\n",
        "        )\n",
        "\n",
        "        h_flip = False\n",
        "\n",
        "        img_sequence = []\n",
        "\n",
        "        # print(images)\n",
        "\n",
        "        if len(images) < 1:\n",
        "\n",
        "            print(os.path.join(self.data_path, path))\n",
        "\n",
        "        bbox = self.bbox.get(path)\n",
        "\n",
        "        if self.augmentation:\n",
        "\n",
        "            ## training set temporal  AUGMENTATION\n",
        "\n",
        "            temporal_augmentation = int(\n",
        "                (np.random.randint(80, 100) / 100.0) * len(images)\n",
        "            )\n",
        "\n",
        "            if temporal_augmentation > 15:\n",
        "\n",
        "                images = sorted(random.sample(images, k=temporal_augmentation))\n",
        "\n",
        "            if len(images) > self.seq_length:\n",
        "\n",
        "                # random frame sampling\n",
        "\n",
        "                images = sorted(random.sample(images, k=self.seq_length))\n",
        "\n",
        "        else:\n",
        "\n",
        "            # test uniform sampling\n",
        "\n",
        "            if len(images) > self.seq_length:\n",
        "\n",
        "                images = sorted(sampling(images, self.seq_length))\n",
        "\n",
        "        i = np.random.randint(0, 30)\n",
        "\n",
        "        j = np.random.randint(0, 30)\n",
        "\n",
        "        brightness = 1 + random.uniform(-0.2, +0.2)\n",
        "\n",
        "        contrast = 1 + random.uniform(-0.2, +0.2)\n",
        "\n",
        "        hue = random.uniform(0, 1) / 10.0\n",
        "\n",
        "        # r_resize = (112, 112)\n",
        "\n",
        "        r_resize = (224, 224)\n",
        "\n",
        "        crop_or_bbox = random.uniform(0, 1) > 0.5\n",
        "\n",
        "        to_flip = random.uniform(0, 1) > 1\n",
        "\n",
        "        grayscale = random.uniform(0, 1) > 0.9\n",
        "\n",
        "        t1 = VideoRandomResizedCrop(self.dim[0], scale=(0.9, 1.0), ratio=(0.8, 1.2))\n",
        "\n",
        "        for img_path in images:\n",
        "\n",
        "            frame_o = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            crop_size = 120\n",
        "\n",
        "            ## CROP BOUNDING BOX\n",
        "\n",
        "            ## CROP BOUNDING BOX\n",
        "\n",
        "            frame1 = np.array(frame_o)\n",
        "\n",
        "            frame1 = frame1[:, crop_size : 648 - crop_size]\n",
        "\n",
        "            frame = Image.fromarray(frame1)\n",
        "\n",
        "            if self.augmentation:\n",
        "\n",
        "                ## training set DATA AUGMENTATION\n",
        "\n",
        "                frame = frame.resize(r_resize)\n",
        "\n",
        "                img_tensor = video_transforms(\n",
        "                    img=frame,\n",
        "                    i=i,\n",
        "                    j=j,\n",
        "                    bright=brightness,\n",
        "                    cont=contrast,\n",
        "                    h=hue,\n",
        "                    dim=self.dim,\n",
        "                    resized_crop=t1,\n",
        "                    augmentation=True,\n",
        "                    normalize=self.normalize,\n",
        "                    crop=crop_or_bbox,\n",
        "                    to_flip=to_flip,\n",
        "                    grayscale=grayscale,\n",
        "                )\n",
        "\n",
        "                img_sequence.append(img_tensor)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # TEST set  NO DATA AUGMENTATION\n",
        "\n",
        "                if is_depth:\n",
        "\n",
        "                    # Resize depth frame to match RGB dimensions\n",
        "\n",
        "                    frame = frame.resize(self.dim)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    frame = frame.resize(self.dim)\n",
        "\n",
        "                img_tensor = video_transforms(\n",
        "                    img=frame,\n",
        "                    i=i,\n",
        "                    j=j,\n",
        "                    bright=1,\n",
        "                    cont=1,\n",
        "                    h=0,\n",
        "                    dim=self.dim,\n",
        "                    augmentation=False,\n",
        "                    normalize=self.normalize,\n",
        "                )\n",
        "\n",
        "                img_sequence.append(img_tensor)\n",
        "\n",
        "        pad_len = self.seq_length - len(images)\n",
        "\n",
        "        X1 = torch.stack(img_sequence).float()\n",
        "\n",
        "        if self.padding:\n",
        "\n",
        "            X1 = pad_video(X1, padding_size=pad_len, padding_type=\"zeros\")\n",
        "\n",
        "        if len(images) < 25:\n",
        "\n",
        "            X1 = pad_video(X1, padding_size=25 - len(images), padding_type=\"zeros\")\n",
        "\n",
        "        return X1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FET8AZPoDb4"
      },
      "outputs": [],
      "source": [
        "class GSLDualViT(pl.LightningModule):\n",
        "    def __init__(self, num_classes, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.class_names = class_names\n",
        "        self.num_classes = len(class_names)\n",
        "        # self.logger = logger\n",
        "        self.learning_rate = config[\"trainer\"][\"optimizer\"][\"lr\"]\n",
        "        self.sequence_length = config[\"dataset\"][\"train\"][\"seq_length\"]\n",
        "\n",
        "        # RGB ViT\n",
        "        self.rgb_vit = ViTModel.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        self.rgb_vit.train()\n",
        "\n",
        "        for param in self.rgb_vit.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # # Depth ViT (initialize with same config but different input channels) # concat better\n",
        "        # depth_config = ViTConfig.from_pretrained(HUGGING_MODEL_NAME)\n",
        "        # depth_config.num_channels = 3\n",
        "        # self.depth_vit = ViTModel(depth_config)\n",
        "\n",
        "        # # Fusion layer\n",
        "        # self.fusion = nn.Linear(\n",
        "        # self.rgb_vit.config.hidden_size * 2, self.rgb_vit.config.hidden_size\n",
        "        # )\n",
        "\n",
        "        # Temporal modeling (remove)  # swap it out with something else (e.g. get feature maps (feature extractor) and save it before you pass to the transformer from backbone efficientnet or resnet and pass to transformer)\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.rgb_vit.config.hidden_size,\n",
        "            self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"hidden_size\"],\n",
        "            num_layers=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"num_layers\"],\n",
        "            bidirectional=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\n",
        "                \"bidirectional\"\n",
        "            ],\n",
        "            batch_first=True,\n",
        "            dropout=self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"dropout\"],\n",
        "        )\n",
        "\n",
        "        # Classification head (need to change)\n",
        "        lstm_output_size = self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\n",
        "            \"hidden_size\"\n",
        "        ]\n",
        "        if self.config[\"trainer\"][\"model\"][\"backbone\"][\"rnn\"][\"bidirectional\"]:\n",
        "            lstm_output_size *= 2\n",
        "\n",
        "        self.classifier = nn.Linear(lstm_output_size, num_classes)\n",
        "\n",
        "    def forward(self, rgb_sequence, depth_sequence=None):\n",
        "        print(\n",
        "            f\"Forward pass input shapes: RGB {rgb_sequence.shape}, Depth {depth_sequence.shape if depth_sequence is not None else None}\"\n",
        "        )\n",
        "        batch_size, seq_len, channels, height, width = rgb_sequence.shape\n",
        "        features = []\n",
        "        for i in range(seq_len):\n",
        "            if (\n",
        "                i == 0 or i == seq_len - 1 or i % 20 == 0\n",
        "            ):  # Log first, last, and every 20th frame\n",
        "                print(f\"Processing frame {i}\")\n",
        "            try:\n",
        "                rgb_features = self.rgb_vit(rgb_sequence[:, i]).last_hidden_state[:, 0]\n",
        "                print(f\"RGB features shape: {rgb_features.shape}\")\n",
        "                # if depth_sequence is not None:\n",
        "                #     depth_features = self.depth_vit(\n",
        "                #         depth_sequence[:, i]\n",
        "                #     ).last_hidden_state[:, 0]\n",
        "                #     print(f\"Depth features shape: {depth_features.shape}\")\n",
        "                #     fused = self.fusion(\n",
        "                #         torch.cat([rgb_features, depth_features], dim=-1)\n",
        "                #     )\n",
        "                # else:\n",
        "                #     fused = rgb_features\n",
        "                # print(f\"Fused features shape: {fused.shape}\")\n",
        "                features.append(rgb_features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {i}: {str(e)}\")\n",
        "        print(f\"Number of features: {len(features)}\")\n",
        "        if len(features) == 0:\n",
        "            print(\"Features is empty!\")\n",
        "            return None\n",
        "        sequence = torch.stack(features, dim=1)\n",
        "        print(f\"Sequence shape: {sequence.shape}\")\n",
        "        lstm_out, _ = self.lstm(sequence)\n",
        "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
        "        final_feature = lstm_out[:, -1, :]\n",
        "        output = self.classifier(final_feature)\n",
        "        print(f\"Final output shape: {output.shape}\")\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # rgb_sequence, depth_sequence, labels = batch\n",
        "        rgb_sequence, labels = batch\n",
        "        print(f\"RGB shape: {rgb_sequence.shape}\")\n",
        "        # print(f\"Depth shape: {depth_sequence.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "        # logits = self(rgb_sequence, depth_sequence)\n",
        "        logits = self(rgb_sequence)\n",
        "        print(f\"Logits shape: {logits.shape if logits is not None else None}\")\n",
        "\n",
        "        if logits is None:\n",
        "            print(\"Logits is None! Check the forward method.\")\n",
        "            return None\n",
        "\n",
        "        # # Assuming you've added a classifier layer to handle the output from the forward method\n",
        "        # # If not, you might need to add: self.classifier = nn.Linear(vit_output_dim, num_classes)\n",
        "        # logits = self.classifier(\n",
        "        #     logits[:, -1]\n",
        "        # )  # Using the last frame's features for classification\n",
        "\n",
        "        # Ensure labels are 1D\n",
        "        labels = labels.view(-1)\n",
        "        labels = labels[: logits.shape[0]]\n",
        "        print(f\"Reshaped Labels shape: {labels.shape}\")\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits, labels) # maybe change to CTC loss\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        self.log(\n",
        "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            \"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # rgb_sequence, depth_sequence, labels = batch\n",
        "        rgb_sequence, labels = batch\n",
        "        # logits = self(rgb_sequence, depth_sequence)\n",
        "        logits = self(rgb_sequence)\n",
        "\n",
        "        labels = labels.view(-1)\n",
        "        labels = labels[: logits.shape[0]]\n",
        "        print(f\"Reshaped Labels shape: {labels.shape}\")\n",
        "\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        self.log(\n",
        "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "        self.log(\n",
        "            \"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.config[\"trainer\"][\"optimizer\"][\"weight_decay\"],\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            factor=self.config[\"trainer\"][\"scheduler\"][\"scheduler_factor\"],\n",
        "            patience=self.config[\"trainer\"][\"scheduler\"][\"scheduler_patience\"],\n",
        "            min_lr=self.config[\"trainer\"][\"scheduler\"][\"scheduler_min_lr\"],\n",
        "            verbose=True,\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"val_loss\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSrp3oXoDb5"
      },
      "outputs": [],
      "source": [
        "def load_config(config_path):\n",
        "    with open(config_path, \"r\") as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "\n",
        "config = load_config(\"configs/dummy.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQYPlpivoDb5"
      },
      "outputs": [],
      "source": [
        "def load_classes(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        classes = [row[0] for row in reader]\n",
        "    return classes\n",
        "\n",
        "\n",
        "class_names = load_classes(config[\"dataset\"][\"classes_filepath\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # rgb_sequences, depth_sequences, labels = zip(*batch)\n",
        "    rgb_sequences, labels = zip(*batch)\n",
        "\n",
        "    rgb_padded = pad_sequence(rgb_sequences, batch_first=True)\n",
        "    # depth_padded = pad_sequence(depth_sequences, batch_first=True)\n",
        "\n",
        "    max_label_length = max(len(label) for label in labels)\n",
        "\n",
        "    padded_labels = []\n",
        "    for label in labels:\n",
        "        padded_label = label.tolist() + [-1] * (max_label_length - len(label))\n",
        "        padded_labels.append(torch.tensor(padded_label))\n",
        "\n",
        "    labels = torch.stack(padded_labels)\n",
        "\n",
        "    print(f\"Collate function: RGB shape: {rgb_padded.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "    # return rgb_padded, depth_padded, labels\n",
        "    return rgb_padded, labels"
      ],
      "metadata": {
        "id": "rVbHYhEaIcLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYQ3dRi4oDb5"
      },
      "outputs": [],
      "source": [
        "args = Args(config)\n",
        "\n",
        "train_dataset = GSL_SI(config, args, mode=\"train\", classes=class_names)\n",
        "val_dataset = GSL_SI(config, args, mode=\"validation\", classes=class_names)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"dataset\"][\"train\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"train\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"train\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"dataset\"][\"validation\"][\"batch_size\"],\n",
        "    shuffle=config[\"dataset\"][\"validation\"][\"shuffle\"],\n",
        "    num_workers=config[\"dataset\"][\"validation\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEqptFd6rqxz"
      },
      "outputs": [],
      "source": [
        "print(f\"Batch size: {train_loader.batch_size}\")\n",
        "print(f\"Workers: {train_loader.num_workers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhY7AWVFoDb5"
      },
      "outputs": [],
      "source": [
        "model = GSLDualViT(num_classes=config[\"dataset\"][\"classes\"], config=config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.compile(model)"
      ],
      "metadata": {
        "id": "rU15GWRY_Si1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRgShDVJoDb5"
      },
      "outputs": [],
      "source": [
        "# trainer setup and initialization\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config[\"trainer\"][\"epochs\"],\n",
        "    # accelerator='cpu' if config['trainer']['cuda'] else 'cpu', # uncomment this line to use CPU, and comment the next line\n",
        "    accelerator=\"gpu\" if config[\"trainer\"][\"cuda\"] else \"cpu\",\n",
        "    devices=1,\n",
        "    precision=\"bf16-mixed\",\n",
        "    num_sanity_val_steps=0,\n",
        "    callbacks=[\n",
        "        ModelCheckpoint(\n",
        "            dirpath=\"/content/drive/Shareddrives/checkpoints/gsl_checkpoints\",\n",
        "            filename=\"gsl-dualvit-{epoch:02d}-{val_loss:.2f}\",\n",
        "            save_top_k=3,\n",
        "            monitor=\"val_loss\",\n",
        "            mode=\"min\",\n",
        "            save_last=True,\n",
        "            every_n_train_steps=1,\n",
        "        ),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=1, mode=\"min\"),\n",
        "    ],\n",
        "    enable_progress_bar=True,\n",
        "    log_every_n_steps=1,\n",
        "    logger=TensorBoardLogger(\"/content/drive/Shareddrives/checkpoints/logs\", name=config[\"trainer\"][\"logger\"]),\n",
        "    enable_checkpointing=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-training"
      ],
      "metadata": {
        "id": "0_WhzvkQBR8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd6dU7SwDmGj"
      },
      "outputs": [],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpmAcWcTY_FN"
      },
      "outputs": [],
      "source": [
        "# # get available memory info\n",
        "# torch.cuda.mem_get_info()\n",
        "\n",
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "\n",
        "\n",
        "# def printm():\n",
        "#     process = psutil.Process(os.getpid())\n",
        "#     print(\n",
        "#         \"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available),\n",
        "#         \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss),\n",
        "#     )\n",
        "#     print(\n",
        "#         \"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(\n",
        "#             gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil * 100, gpu.memoryTotal\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "\n",
        "# # free up gpu memory before training\n",
        "# def free_gpu_cache():\n",
        "\n",
        "#     print(\"Initial GPU Usage\")\n",
        "#     gpu_usage()\n",
        "\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     cuda.select_device(0)\n",
        "\n",
        "#     cuda.close()\n",
        "\n",
        "#     cuda.select_device(0)\n",
        "\n",
        "#     print(\"GPU Usage after emptying the cache\")\n",
        "#     gpu_usage()\n",
        "\n",
        "\n",
        "# free_gpu_cache()\n",
        "# printm()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "73Gy55r7A8NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfLDtu0FCS6Z"
      },
      "outputs": [],
      "source": [
        "# model.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvjg3cuMJSBW"
      },
      "outputs": [],
      "source": [
        "!sudo chmod -R 777 logs/train_CSLR/\n",
        "!sudo chown -R $(whoami) logs/train_CSLR/\n",
        "!sudo chown root:root logs/train_CSLR/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo chmod -R 777 /content/drive/Shareddrives/checkpoints\n",
        "!sudo chown -R $(whoami) /content/drive/Shareddrives/checkpoints\n",
        "!sudo chown root:root /content/drive/Shareddrives/checkpoints"
      ],
      "metadata": {
        "id": "BLs3EmvnUsBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")"
      ],
      "metadata": {
        "id": "Q2qyuJGO2JJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "p8Hjr2eZBaRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrLxUQ0FoDb6"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "# trainer.fit(model, train_loader, val_loader, ckpt_path=\"/content/drive/Shareddrives/checkpoints/gsl_checkpoints/last.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary"
      ],
      "metadata": {
        "id": "9rHUd9gaCWdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = model\n",
        "modules = [module for module in net.modules()]\n",
        "params = [param.shape for param in net.parameters()]\n",
        "\n",
        "print(modules[0])\n",
        "total_params=0\n",
        "for i in range(1,len(modules)):\n",
        "   j = 2*i\n",
        "   param = (params[j-2][1]*params[j-2][0])+params[j-1][0]\n",
        "   total_params += param\n",
        "   print(\"Layer\",i,\"->\\t\",end=\"\")\n",
        "   print(\"Weights:\", params[j-2][0],\"x\",params[j-2][1],\n",
        "         \"\\tBias: \",params[j-1][0], \"\\tParameters: \", param)\n",
        "print(\"\\nTotal Params: \", total_params)"
      ],
      "metadata": {
        "id": "4GBBl1Ip8vMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "batch_size = 1\n",
        "summary(model, input_size=(batch_size, 1, 28, 28))"
      ],
      "metadata": {
        "id": "OwnOdARv9izy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on test"
      ],
      "metadata": {
        "id": "6avgG4hWcwoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pass"
      ],
      "metadata": {
        "id": "fz9tBvJScyVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "7VV_OEPgdH9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pass"
      ],
      "metadata": {
        "id": "AewuW3dAdJok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation"
      ],
      "metadata": {
        "id": "tcRECqzxdKCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix and f1 score\n",
        "pass"
      ],
      "metadata": {
        "id": "3F1pL7c7dLOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot accuracy against epoch (training, test, dev)\n",
        "pass"
      ],
      "metadata": {
        "id": "U2Zlx6QgCoic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss against epoch (training, test, dev)\n",
        "pass"
      ],
      "metadata": {
        "id": "-01BCMVnCy59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to Hugging Face"
      ],
      "metadata": {
        "id": "15hBN646coE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/Shareddrives/checkpoints/gsl_checkpoints\")"
      ],
      "metadata": {
        "id": "Qkg5De9_CP7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "kPRzQPIAcpB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gsl-dualvit\"\n",
        "api = HfApi()\n",
        "repo_id = f\"fawxyz/{model_name}\"\n",
        "try:\n",
        "    api.create_repo(repo_id)\n",
        "    print(f\"Repo {repo_id} created\")\n",
        "except:\n",
        "    print(f\"Repo {repo_id} already exists\")"
      ],
      "metadata": {
        "id": "NyPopM0scqFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(\n",
        "    folder_path=model_name, path_in_repo=\".\", repo_id=repo_id, repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "0wCMrssYcrym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing model from Hugging Face"
      ],
      "metadata": {
        "id": "R5Ikarv4Bkhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "import requests\n",
        "from IPython.display import display\n",
        "\n",
        "url = \"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-12638-3_33/MediaObjects/527350_1_En_33_Fig11_HTML.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('fawxyz/gsl-dualvit')\n",
        "model = ViTForImageClassification.from_pretrained('fawxyz/gsl-dualvit')\n",
        "\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "# model predicts one of the 1000 ImageNet classes\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "id": "zJ-aWD9z_uAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-12638-3_33/MediaObjects/527350_1_En_33_Fig11_HTML.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "print(\"Original Image:\")\n",
        "display(image)\n",
        "\n",
        "image_size = (224, 224)\n",
        "image_resized = image.resize(image_size)\n",
        "\n",
        "print(\"Resized Image:\")\n",
        "display(image_resized)\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('fawxyz/gsl-dualvit')\n",
        "model = ViTForImageClassification.from_pretrained('fawxyz/gsl-dualvit')\n",
        "\n",
        "image_array = np.array(image_resized.convert('RGB'))\n",
        "\n",
        "image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()\n",
        "\n",
        "image_tensor = image_tensor.unsqueeze(0) / 255.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image_tensor)\n",
        "\n",
        "logits = outputs.logits\n",
        "\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "predicted_class_label = model.config.id2label[predicted_class_idx]\n",
        "\n",
        "print(f\"\\nPredicted class index: {predicted_class_idx}\")\n",
        "print(f\"Predicted class label: {predicted_class_label}\")\n",
        "\n",
        "top_5 = logits.topk(5)\n",
        "for i in range(5):\n",
        "    class_idx = top_5.indices[0][i].item()\n",
        "    class_label = model.config.id2label[class_idx]\n",
        "    probability = torch.softmax(logits, dim=1)[0][class_idx].item()\n",
        "    print(f\"Top {i+1}: {class_label} (Index: {class_idx}, Probability: {probability:.4f})\")"
      ],
      "metadata": {
        "id": "FgAmqnxmAe--"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}